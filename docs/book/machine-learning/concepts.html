
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Machine Learning Concepts &#8212; Data Science Hand.. 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          Handbook</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Section <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Machine Learning Concepts</a><ul>
<li><a class="reference internal" href="#discriminative-vs-generative">Discriminative vs Generative</a></li>
<li><a class="reference internal" href="#bias-variance-trade-off">Bias Variance Trade-off</a></li>
<li><a class="reference internal" href="#overfitting">Overfitting</a></li>
<li><a class="reference internal" href="#regularization">Regularization</a></li>
<li><a class="reference internal" href="#cross-validation">Cross Validation</a></li>
<li><a class="reference internal" href="#distance-measurements">Distance Measurements</a></li>
<li><a class="reference internal" href="#loss-functions">Loss Functions</a><ul>
<li><a class="reference internal" href="#binary-cross-entropy">Binary Cross Entropy</a></li>
<li><a class="reference internal" href="#categorical-cross-entropy">Categorical Cross Entropy</a></li>
<li><a class="reference internal" href="#kldivergence">KLDivergence</a></li>
<li><a class="reference internal" href="#mean-absolute-error">Mean Absolute Error</a></li>
<li><a class="reference internal" href="#mean-absolute-percentage-error">Mean Absolute Percentage Error</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="machine-learning-concepts">
<h1>Machine Learning Concepts<a class="headerlink" href="#machine-learning-concepts" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<div class="section" id="discriminative-vs-generative">
<h2>Discriminative vs Generative<a class="headerlink" href="#discriminative-vs-generative" title="Permalink to this headline"></a></h2>
<p><img alt="image" src="../../_images/discrimitive_generative.png" /></p>
</div>
<div class="section" id="bias-variance-trade-off">
<h2>Bias Variance Trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Bias:</strong> is the difference between the expected (or average) prediction of
our model and the  correct value which we are trying to predict.</p></li>
<li><p><strong>Variance:</strong> is the variability of a model prediction for a given data point.</p></li>
<li><p>The sweet spot for any model is the level of complexity at which the increase
in bias is equivalent  to the reduction in variance.</p></li>
<li><p>Increasing model complexity tends to increase variance and decrease bias.</p></li>
<li><p>However our model complexity exceeds this sweet spot we are in effect
over-fitting; while if our complexity falls short of the sweet spot =
under-fitting</p></li>
</ul>
<p><img alt="image" src="../../_images/bias_variance.png" /></p>
<p><strong>Addressing Variance:</strong></p>
<ul class="simple">
<li><p>Bagging and other resampling techniques can be used to reduce the variance in
model predictions.</p></li>
<li><p>In bagging (Bootstrap Aggregating), numerous replicates of the original data
set are created using random selection with replacement.</p></li>
</ul>
</div>
<div class="section" id="overfitting">
<h2>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Occurs when the model over fits on the training data and does not generalize
to the unseen sample population.</p></li>
<li><p>The model performs better on the training data than it does on data it has
never seen before.</p></li>
<li><p>After this point, the model over-optimizes and learns representations
specific to the training data that do not generalize to test data.</p></li>
</ul>
<p><img alt="image" src="../../_images/overfitting.png" /></p>
<p><strong>Ways to address overfitting:</strong></p>
<ul class="simple">
<li><p>Get more data</p></li>
<li><p>Add early stopping (epochs)</p></li>
<li><p>Add regularization</p></li>
<li><p>Cross-Validation</p></li>
<li><p>Less complex model</p></li>
<li><p>Data augmentation (images)</p></li>
<li><p>Smaller input dimensionality (remove features)</p></li>
</ul>
</div>
<div class="section" id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Technique to help reduce overfitting by adding an additional parameter to the
loss function, usually  the <code class="docutils literal notranslate"><span class="pre">L1</span></code> or <code class="docutils literal notranslate"><span class="pre">L2</span></code> norm.</p></li>
<li><p>In order to help prevent overfitting, we can add in a term into our
optimization that keeps the weights small.</p></li>
</ul>
<p><strong>L1 Regularization (Lasso): “Absolute Value Magnitude”</strong></p>
<ul class="simple">
<li><p>Lasso Regularizer forces a lot of feature weights to be zero</p></li>
</ul>
<p><strong>L2 Regularization (Ridge): “Squared Magnitude”</strong></p>
<p><img alt="image" src="../../_images/regularization.png" /></p>
</div>
<div class="section" id="cross-validation">
<h2>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>To avoid sampling issues, which can cause the training-set to be too optimistic.</p></li>
<li><p>Cross-validation is used to protect against overfitting in a predictive
model, particularly the case where the amount of data is limited.</p></li>
</ul>
<p><strong>K-Fold:</strong></p>
<ul class="simple">
<li><p>Splits the training data into <code class="docutils literal notranslate"><span class="pre">k-folds</span></code> to validate the model on one file
while training on the k-1 other folds ‘k’ times.</p></li>
<li><p>The error is then averages over the fold.</p></li>
</ul>
</div>
<div class="section" id="distance-measurements">
<h2>Distance Measurements<a class="headerlink" href="#distance-measurements" title="Permalink to this headline"></a></h2>
<p><strong>Euclidean Distance</strong></p>
<ul class="simple">
<li><p>sqrt((x2-x1)2 + (y2-y2)2)–&gt; Pythagorean Theorem</p></li>
</ul>
<p><strong>Manhattan Distance</strong></p>
<ul class="simple">
<li><p>Calculates the distance between two data points in a grid like path -
absolute sum of difference.</p></li>
</ul>
<p><strong>Cosine Distance</strong></p>
<ul class="simple">
<li><p>Measure the degree of angle between two documents or vectors.</p></li>
<li><p>Cosine value 1 is for vectors pointing in the same direction i.e. there are
similarities between the documents/data points.</p></li>
<li><p>At zero for orthogonal vectors -&gt; meaning unrelated (some similarity found).</p></li>
</ul>
<p><strong>Mahalanobis Distance</strong></p>
<ul class="simple">
<li><p>A measure of the distance between a point P and a distribution D.</p></li>
<li><p><strong>Why use it?</strong></p>
<ul>
<li><p>If the feature vectors are correlated to one another, which is typically
the case in real-world datasets, the <code class="docutils literal notranslate"><span class="pre">Euclidean</span> <span class="pre">distance</span></code> between a point
and the center of the points (distribution) can give little or misleading
information about how close a point really is to the cluster.</p></li>
<li><p>Euclidean distance is a distance between two points only. It does not
consider how the rest of  the points in the dataset vary</p></li>
</ul>
</li>
<li><p><strong>Steps:</strong></p>
<ul>
<li><p>It transforms the columns into uncorrelated variables</p></li>
<li><p>Scale the columns to make their variance equal to 1</p></li>
<li><p>Finally, it calculates the Euclidean distance.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>A model needs a <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code> and an <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> for training.</p></li>
<li><p>Function that takes as inputs the predicted value ‘z’ corresponding to the
real data value ‘y’ and outputs how different they are.</p></li>
<li><p>See below for examples of loss functions.</p></li>
</ul>
<p><img alt="image" src="../../_images/loss_functions.png" /></p>
<div class="section" id="binary-cross-entropy">
<h3>Binary Cross Entropy<a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Binary Classification problems</p></li>
<li><p>For a binary classification problem the model outputs a probability</p></li>
<li><p>Therefore binary-cross entropy is better for dealing with probabilities as it
measures the <code class="docutils literal notranslate"><span class="pre">distance</span></code> between probability distributions, or in our case,
between the ground-truth distribution and the predictions.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)])</span>
</pre></div>
</div>
<p><strong>Entropy</strong>: recall that entropy is the number of bits required to transmit a
randomly selected event from a probability distribution. A skewed distribution
has a low entropy, whereas a distribution where events have equal probability
has a larger entropy.</p>
<p><img alt="image" src="../../_images/cross_entropy1.png" />
<img alt="image" src="../../_images/cross_entropy2.png" /></p>
</div>
<div class="section" id="categorical-cross-entropy">
<h3>Categorical Cross Entropy<a class="headerlink" href="#categorical-cross-entropy" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Classification problems</p></li>
<li><p>Computes the cross entropy loss between the labels and predictions.</p></li>
<li><p>If you want to provide labels using one-hot representation, use this loss
function.</p></li>
</ul>
</div>
<div class="section" id="kldivergence">
<h3>KLDivergence<a class="headerlink" href="#kldivergence" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Used to approximate a more complex function than simply multi-class
classification like <code class="docutils literal notranslate"><span class="pre">Autoencoders</span></code>.</p></li>
<li><p>Computes <code class="docutils literal notranslate"><span class="pre">Kullback-Leibler</span></code> or <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">Divergence</span></code> measures the loss between y_true and y_pred.</p></li>
<li><p>KL Divergence is a measure of how one probability distribution differs from a
baseline distribution.
A KL divergence <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">of</span> <span class="pre">0</span></code> suggests the distributions are identical.</p></li>
<li><p>The behavior of KL Divergence is very similar to cross-entropy.</p></li>
<li><p>It calculates how much information is lost if the predicted probability
distribution is used to approximate the desired target probability
distribution.</p></li>
</ul>
</div>
<div class="section" id="mean-absolute-error">
<h3>Mean Absolute Error<a class="headerlink" href="#mean-absolute-error" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Regression problems</p></li>
<li><p>Computes the mean of absolute difference between labels and predictions.</p></li>
</ul>
</div>
<div class="section" id="mean-absolute-percentage-error">
<h3>Mean Absolute Percentage Error<a class="headerlink" href="#mean-absolute-percentage-error" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Regression problems</p></li>
<li><p>Computes the mean absolute percentage error between y_true and y_pred.</p></li>
</ul>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/book/machine-learning/concepts.md.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2021, Trace Smith, Damon Resnick.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>