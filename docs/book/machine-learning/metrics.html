
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Model Evaluation &#8212; Data Science Hand.. 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          Handbook</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Section <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Model Evaluation</a></li>
<li><a class="reference internal" href="#regression">Regression</a><ul>
<li><a class="reference internal" href="#r-squared">R-Squared</a></li>
<li><a class="reference internal" href="#adjusted-r-squared">Adjusted R-Squared</a></li>
<li><a class="reference internal" href="#root-mean-squared-error">Root Mean Squared Error</a></li>
<li><a class="reference internal" href="#mean-absolute-error">Mean Absolute Error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">Classification</a><ul>
<li><a class="reference internal" href="#accuracy">Accuracy</a></li>
<li><a class="reference internal" href="#precision">Precision</a></li>
<li><a class="reference internal" href="#recall">Recall</a></li>
<li><a class="reference internal" href="#f1-score">F1 Score</a></li>
<li><a class="reference internal" href="#specificity">Specificity</a></li>
<li><a class="reference internal" href="#false-positives">False positives</a></li>
<li><a class="reference internal" href="#false-negatives">False negatives</a></li>
<li><a class="reference internal" href="#auc-score">AUC Score</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="model-evaluation">
<h1>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline"></a></h1>
</div>
<hr class="docutils" />
<div class="section" id="regression">
<h1>Regression<a class="headerlink" href="#regression" title="Permalink to this headline"></a></h1>
<div class="section" id="r-squared">
<h2>R-Squared<a class="headerlink" href="#r-squared" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Statistical measure of fit that indicates how much variation of a dependent
variable is explained by  the independent variable(s) in a regression model.</p></li>
<li><p><strong>Problem 1:</strong> Every time you add a predictor to a model, the R-squared
increases, even if due to chance alone. It never decreases. Consequently, a
model with more terms may appear to have a better fit simply because it has
more terms.</p></li>
<li><p><strong>Problem 2:</strong> If a model has too many predictors and higher order
polynomials, it begins to model  the random noise in the data.
Leads to overfitting and  produces misleadingly high R-squared values and a
lessened ability to make  predictions.</p></li>
</ul>
</div>
<div class="section" id="adjusted-r-squared">
<h2>Adjusted R-Squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Modified version of R-squared that has been adjusted for the number of
predictors in the model.</p></li>
<li><p>Increases only if the new term improves the model more than would be expected
by chance.</p></li>
<li><p>Decreases when a predictor improves the model by less than expected by chance.</p></li>
<li><p>The adjusted R-squared can be negative, but it’s usually not.</p></li>
<li><p>It is always lower than the R-squared.</p></li>
</ul>
</div>
<div class="section" id="root-mean-squared-error">
<h2>Root Mean Squared Error<a class="headerlink" href="#root-mean-squared-error" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The square root of the variance of the residuals. It indicates the absolute
fit of the model to the  data–how close the observed data points are to the
model’s predicted values.</p></li>
<li><p>Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit.</p></li>
<li><p>As the square root of a variance, RMSE can be interpreted as the standard
deviation of the unexplained variance, and has the useful property of being
in the same units as the response variable.</p></li>
</ul>
<p><strong>Lower values of RMSE indicate better fit</strong></p>
<ul class="simple">
<li><p>RMSE is a good measure of how accurately the model predicts the response, and
it is the most important criterion for fit if the main purpose of the model
is prediction.</p></li>
</ul>
</div>
<div class="section" id="mean-absolute-error">
<h2>Mean Absolute Error<a class="headerlink" href="#mean-absolute-error" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>MAE is not identical to root-mean square error (RMSE), although some
researchers report and interpret it that way.</p></li>
<li><p>MAE is conceptually simpler and also easier to interpret than RMSE: it is
simply the average  absolute vertical or horizontal distance between each
point in a scatter plot and the Y=X line.</p></li>
<li><p>In other words, MAE is the average absolute difference between X and Y.</p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="classification">
<h1>Classification<a class="headerlink" href="#classification" title="Permalink to this headline"></a></h1>
<div class="section" id="accuracy">
<h2>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Accuracy = (TP+TN)/(TP+FP+FN+TN)</p></li>
<li><p>Accuracy is a valid choice of evaluation for classification problems which
are well balanced and not  skewed or No class imbalance.</p></li>
<li><p><strong>Warning:</strong> Let us say that our target class is very sparse. Do we want
accuracy as a metric of our  model performance? What if we are predicting if
an asteroid will hit the earth? Just say No all the time. And you will be 99%
accurate.</p></li>
</ul>
</div>
<div class="section" id="precision">
<h2>Precision<a class="headerlink" href="#precision" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Positive</span> <span class="pre">Predictive</span> <span class="pre">Rate</span></code></p></li>
<li><p>Precision = (TP)/(TP+FP)</p></li>
<li><p>What proportion of predicted Positives is truly Positive?</p></li>
<li><p>Precision is a valid choice of evaluation metric when we want to be very sure
of our prediction</p></li>
</ul>
</div>
<div class="section" id="recall">
<h2>Recall<a class="headerlink" href="#recall" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span> <span class="pre">Positives</span></code> or <code class="docutils literal notranslate"><span class="pre">Sensitivity</span></code></p></li>
<li><p>These are cases in which mode predicted True and label is True</p></li>
<li><p><strong>Example:</strong> When the label is True, how often does the model predict True?</p></li>
</ul>
</div>
<div class="section" id="f1-score">
<h2>F1 Score<a class="headerlink" href="#f1-score" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Weighted average of the recall and precision</p></li>
<li><p>F1 score sort of maintains a balance between the precision and recall for
your classifier.</p>
<ul>
<li><p>If your precision is low, the F1 is low and if the recall is low again
your F1 score is low.</p></li>
</ul>
</li>
<li><p><strong>Example:</strong> If you are a police inspector and you want to catch criminals,
you want to be sure that  the person you catch is a criminal (Precision) and
you also want to capture as many criminals (Recall) as possible. The F1 score
manages this tradeoff.</p></li>
</ul>
</div>
<div class="section" id="specificity">
<h2>Specificity<a class="headerlink" href="#specificity" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span> <span class="pre">negative</span></code> = (1 - False Negative Rate)</p></li>
<li><p>Model predicted no and label is False.</p></li>
<li><p><strong>Example:</strong> When the label is False, how often does the model predict False</p></li>
</ul>
</div>
<div class="section" id="false-positives">
<h2>False positives<a class="headerlink" href="#false-positives" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Type</span> <span class="pre">1</span> <span class="pre">Error</span></code></p></li>
<li><p><strong>Example:</strong> Model predicted yes, but the patient didn’t actually have the disease.</p></li>
</ul>
</div>
<div class="section" id="false-negatives">
<h2>False negatives<a class="headerlink" href="#false-negatives" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Type</span> <span class="pre">II</span> <span class="pre">Error</span></code></p></li>
<li><p><strong>Example:</strong> Model predicted no, but the patient actually did have the disease.</p></li>
</ul>
</div>
<div class="section" id="auc-score">
<h2>AUC Score<a class="headerlink" href="#auc-score" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>AUC measures the entire two-dimensional area underneath the entire ROC curve
( TPR and FPR)</p></li>
<li><p>It tells how much a model is capable of distinguishing between classes.</p></li>
<li><p>Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.</p></li>
<li><p><strong>Example:</strong> Higher the AUC, then the better the model is at distinguishing
between patients with  disease and no disease.</p></li>
</ul>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/book/machine-learning/metrics.md.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2021, Trace Smith, Damon Resnick.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>