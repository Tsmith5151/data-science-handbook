<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Neural Networks (RecSys) &mdash; Data Science Hand.. 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Data Science Hand..
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Deep Neural Networks (RecSys)</a><ul>
<li><a class="reference internal" href="#softmax-model">SoftMax Model</a></li>
<li><a class="reference internal" href="#embedding-layers">Embedding Layers</a></li>
<li><a class="reference internal" href="#candidate-generator-model">Candidate Generator Model</a></li>
<li><a class="reference internal" href="#ranking-model">Ranking Model</a></li>
<li><a class="reference internal" href="#neural-collaborative-filtering">Neural Collaborative Filtering</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Data Science Hand..</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Deep Neural Networks (RecSys)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/book/recommendation-engines/dnn.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="deep-neural-networks-recsys">
<h1>Deep Neural Networks (RecSys)<a class="headerlink" href="#deep-neural-networks-recsys" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<p><strong>Overview</strong></p>
<ul class="simple">
<li><p>Deep neural network (DNN) models can address these limitations of matrix factorization.</p></li>
<li><p>By adding hidden layers and non-linear activation functions (for example,
ReLU), the model can capture more complex relationships in the data.</p></li>
<li><p>DNNs can easily incorporate query features and item features (due to the
flexibility of the input layer of the network), which can help capture the
specific interests of a user and improve the relevance of recommendations.</p></li>
</ul>
<p><img alt="image" src="../../_images/dnn_recsys.png" /></p>
<p><strong>Matrix Factorization vs DNN</strong></p>
<ul class="simple">
<li><p>In both the softmax model and the matrix factorization model, the system
learns one embedding vector <code class="docutils literal notranslate"><span class="pre">Vj</span></code> per item <code class="docutils literal notranslate"><span class="pre">j</span></code>.</p></li>
<li><p>What is called the item embedding matrix in matrix factorization is
now the matrix of weights of the softmax layer.</p></li>
<li><p>The query embeddings, however, are different.</p></li>
<li><p>Instead of learning one embedding Ui per query i, the system learns a mapping
from the query feature x to an embedding space.</p></li>
<li><p>Therefore, you can think of this DNN model as a generalization of matrix
factorization, in which you replace the query side by a <code class="docutils literal notranslate"><span class="pre">nonlinear</span> <span class="pre">function</span></code>.</p></li>
<li><p>DNN models solve many limitations of Matrix Factorization, but are typically
more expensive to train and query.</p></li>
</ul>
<p><img alt="image" src="../../_images/softmax_recsys2.png" /></p>
<div class="section" id="softmax-model">
<h2>SoftMax Model<a class="headerlink" href="#softmax-model" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>One possible DNN model is softmax, which treats the problem as a multiclass
prediction problem in which:</p></li>
<li><p>The input is the user query.</p></li>
<li><p>The output is a probability vector with size equal to the number of items in
the corpus, representing the probability to interact with each item; for
example, the probability to click on or watch a YouTube video.</p></li>
<li><p><strong>Inputs:</strong></p>
<ul>
<li><p>dense features (for example, watch time and time since last watch)</p></li>
<li><p>sparse features (for example, watch history and country)</p></li>
</ul>
</li>
</ul>
<p><strong>Note:</strong> You can think of this DNN model as a generalization of matrix
factorization, in which you replace the query side by a nonlinear function</p>
<p><img alt="image" src="../../_images/softmax_recsys.png" /></p>
</div>
<div class="section" id="embedding-layers">
<h2>Embedding Layers<a class="headerlink" href="#embedding-layers" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Embedding layers is a relatively low-dimensional space into which you can
translate high-dimensional vectors.</p></li>
<li><p>Embeddings make it easier to do machine learning on large inputs like sparse vectors.</p></li>
<li><p>Embedding can basically be thought of as look-up tables</p></li>
<li><p>Embedding layers capture some of the semantics of an input by placing
semantically similar inputs close together in the embedding space.</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="candidate-generator-model">
<h2>Candidate Generator Model<a class="headerlink" href="#candidate-generator-model" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>In this first stage, the system starts from a potentially huge corpus and
generates a much smaller subset of candidates.</p></li>
<li><p>It is about selecting an initial set of hundreds of candidates from all
possible candidates. The  main objective of this model is to efficiently weed
out all candidates that the user is not interested in</p></li>
<li><p>For example, the candidate generator in YouTube reduces billions of videos
down to hundreds or thousands.</p></li>
<li><p>The model needs to evaluate queries quickly given the enormous size of the corpus.</p></li>
<li><p>A retrieval system is a model that predicts a set of movies from the
catalogue that the user is likely to watch. So the train set should be
expressing which movies the users watched, and which they did not.</p></li>
<li><p>The similarity between the query representation (query embedding vector) and
the candidate representation (candidate embedding vector) a.k.a. affinity
score can be calculated by dot-product (or other similarity measures). The
K-nearest candidates (candidates with higher affinity scores) will be chosen
for the final list.</p></li>
<li><p>Let’s say in our training data we only have positive (user, items) pairs.</p></li>
<li><p>To figure out how good our model is, we need to compare the affinity score
that the model calculates for this positive pair to the scores of all the
other possible candidates</p></li>
<li><p>If the score for the positive pair is higher than for all other possible
candidates, our model is highly accurate.</p></li>
<li><p>To measure the performance of a retrieval task, <code class="docutils literal notranslate"><span class="pre">factorized</span> <span class="pre">top-K</span> <span class="pre">categorical</span></code> accuracy metrics over a corpus of candidates can be used. These
metrics measure how good the model is at picking the true candidate out of
all possible candidates in the system.</p></li>
</ul>
<p><strong>Metrics</strong></p>
<ul class="simple">
<li><p><strong>factorized_top_k.TopK</strong>: which computes the top K categorical accuracy.</p></li>
<li><p>How often is the true candidate in the top K candidates for a given query?</p></li>
<li><p>As the model trains, the top-k retrieval metrics updates.</p></li>
<li><p>The factorized_top_k retrieval metric measures the number of true positive
that are in the top-k  retrieved items from the entire candidate set.</p></li>
<li><p><strong>Example</strong>: a top-5 categorical accuracy metric of 0.2 would tell us that,
on average, the true  positive is in the top 5 retrieved items 20% of the
time.</p></li>
</ul>
<p>To compute the nearest neighbors in the embedding space, the system can
exhaustively score every potential candidate.</p>
<ul class="simple">
<li><p>Exhaustive scoring can be expensive for very large corpora, but you can use
either of the following
strategies to make it more efficient.</p></li>
<li><p>If the query embedding is known statically (e.g. learned weights), the
system can perform exhaustive scoring offline, precomputing and storing a
list of the top candidates for each query. This is a common practice for
related-item recommendation.</p></li>
</ul>
<p><strong>Methods:</strong></p>
<ul class="simple">
<li><p>Brute-Force</p></li>
<li><p>ANN (Approximate Nearest Neighbor)</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="ranking-model">
<h2>Ranking Model<a class="headerlink" href="#ranking-model" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Finally, the system must take into account additional constraints for the final ranking.</p></li>
<li><p>For example, the system removes items that the user explicitly disliked or
boosts the score of fresher content.</p></li>
<li><p>Re-ranking can also help ensure diversity, freshness, and fairness.</p></li>
</ul>
<p><img alt="image" src="../../_images/ranking_metric.png" /></p>
<p><strong>Diversity:</strong></p>
<ul class="simple">
<li><p>If the system always recommends items that are “closest” to the query
embedding, the candidates tend to be very similar to each other. This lack
of diversity can cause a bad or boring user experience.</p></li>
</ul>
<p><strong>TopK Categorical Accuracy</strong></p>
<ul class="simple">
<li><p>Calculates the percentage of records for which the targets Y_true are in the
top K predictions  (Y_pred).</p></li>
<li><p>We rank the Y_pred predictions in the descending order of probability values.</p></li>
<li><p>If the rank of the Y_pred present in the index of Y_true is less than or equal to K, it is considered accurate.</p></li>
<li><p>We then calculate TopK Categorical Accuracy by dividing the number of
accurately predicted records by the total number of records.</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="neural-collaborative-filtering">
<h2>Neural Collaborative Filtering<a class="headerlink" href="#neural-collaborative-filtering" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>One drawback of using implicit feedback is that there is a natural scarcity
for negative feedback.</p></li>
<li><p>By employing a probabilistic treatment, NCF transforms the recommendation
problem to a binary classification problem</p></li>
<li><p>To account for negative instances y- is uniformly sampled from the unobserved
interactions.</p></li>
<li><p>NCF has 2 components GMF and MLP with the following benefitsGMF that applies
the linear kernel to model user-item interactions like vanilla MF.</p></li>
<li><p>MLP that uses multiple neural layers to layer nonlinear interactions</p></li>
<li><p>NCF combines these models together to superimpose their desirable
characteristics. NCF concatenates the output of GMF and MLP before feeding
them into NeuMF layer.</p></li>
</ul>
<p><img alt="image" src="../../_images/ncf.png" /></p>
<p><img alt="image" src="../../_images/ncf2.png" /></p>
<p><strong>Things to know for Neural CF:</strong></p>
<ul class="simple">
<li><p>GMF/MLP have separate user and item embeddings. This is to make sure that
both of them learn optimal  embeddings independently.</p></li>
<li><p>GMF replicates the vanilla MF by element-wise product of the user-item vector.</p></li>
<li><p>MLP takes the concatenation of user-item latent vectors as input.</p></li>
<li><p>The outputs of GMF and MLP are concatenated in the final NeuMF(Neural Matrix
Factorisation) layer.</p></li>
</ul>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>NCF is an example of multimodal deep learning as it contains data from 2
pathways namely user and item.</p></li>
<li><p>The most intuitive way to combine them is by concatenation.</p></li>
<li><p>But a simple vector concatenation does not account for user-item interactions
and is insufficient to model the collaborative filtering effect.</p></li>
<li><p>To address this NCF adds hidden layers on top of concatenated user-item
vectors(MLP framework), to learn user-item interactions.</p></li>
<li><p>This endows the model with a lot of flexibility and non-linearity to learn
the user-item interactions.</p></li>
<li><p>This is an upgrade over MF that uses a fixed element-wise product on them.</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>