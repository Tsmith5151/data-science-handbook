
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sequence Modeling &#8212; Data Science Hand.. 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          Handbook</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Section <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Sequence Modeling</a><ul>
<li><a class="reference internal" href="#recurrent-neural-networks">Recurrent Neural Networks</a></li>
<li><a class="reference internal" href="#bidirectional-lstm">Bidirectional LSTM</a></li>
<li><a class="reference internal" href="#elmo">ELMo</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="sequence-modeling">
<h1>Sequence Modeling<a class="headerlink" href="#sequence-modeling" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<div class="section" id="recurrent-neural-networks">
<h2>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Recall one of the issues with word embeddings is they are
context-independent. <code class="docutils literal notranslate"><span class="pre">RNNs</span></code> can be leveraged to help
contextualize embeddings and words can be disambiguated based on their context.</p></li>
</ul>
<p><strong><span class="label label-warning">Disadvantages</span></strong></p>
<ul class="simple">
<li><p>Slow: O(N) is the number of tokens.</p></li>
<li><p>Vanishing Gradient: cannot process long sentences.</p></li>
<li><p>Unidirectional: process text left to right.</p></li>
</ul>
<p><img alt="image" src="../../_images/sequence_model.png" />
<a class="reference external" href="https://www.youtube.com/watch?v=LE3NfEULV6k">Reference</a></p>
</div>
<div class="section" id="bidirectional-lstm">
<h2>Bidirectional LSTM<a class="headerlink" href="#bidirectional-lstm" title="Permalink to this headline"></a></h2>
<p><strong>Note</strong>: The training is called bidirectional language model (biLM) that can
learn from the past and predict the next word in a sequence of words like a
sentence.</p>
<p><strong>Difference between BI-LSTM vs LSTM:</strong></p>
<ul class="simple">
<li><p>Unidirectional LSTM only preserves information of the past because the only
inputs it has seen are from the past.
Using bidirectional will run your inputs in two ways, one from past to future
and one from future to past and what differs this approach from
unidirectional is that in the LSTM that runs backwards you preserve
information from the future and using the two hidden states combined you are
able in any point in time to preserve information from both past and future.</p></li>
</ul>
</div>
<div class="section" id="elmo">
<h2>ELMo<a class="headerlink" href="#elmo" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>In short:</strong> ELMo attempts to dynamically assign a vector to a word, based
on the sentence within which it’s contained. Instead of a look-up table with
a word and an embedding like Word2Vec, the ELMo model lets users input text
into the model, and it generates an embedding based on that sentence. Thus,
it can generate different meanings for a word depending on the context.</p></li>
<li><p>ELMo (2018), a new technique for embedding words into real vector space using
bidirectional LSTMs trained on a language modeling objective.</p></li>
<li><p>ELMo gained its language understanding from being trained to predict the next
word in a sequence of words.<br />This is convenient because we have vast amounts of text data that such a
model can learn from without needing labels.</p></li>
<li><p>Instead of using a fixed embedding for each word, ELMo looks at the entire
sentence before assigning each word in it an embedding. It uses a
bi-directional LSTM trained on a specific task to be able to create those
embeddings.</p></li>
<li><p>ELMo has a great understanding of the language because it’s trained on a
massive dataset, ELMo embeddings are trained on the 1 Billion Word Benchmark.</p></li>
</ul>
<p><img alt="image" src="../../_images/elmo.png" /></p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/book/nlp/sequence.md.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2021, Trace Smith, Damon Resnick.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>