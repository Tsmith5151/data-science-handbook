
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>NLP Concepts &#8212; Data Science Hand.. 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          Handbook</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Section <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">NLP Concepts</a><ul>
<li><a class="reference internal" href="#text-preprocessing">Text Preprocessing</a></li>
<li><a class="reference internal" href="#tokenization">Tokenization</a></li>
<li><a class="reference internal" href="#tfidf">TFIDF</a></li>
<li><a class="reference internal" href="#bag-of-words">Bag of Words</a></li>
<li><a class="reference internal" href="#skip-grams">Skip Grams</a></li>
<li><a class="reference internal" href="#named-entity-recognition">Named Entity Recognition</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="nlp-concepts">
<h1>NLP Concepts<a class="headerlink" href="#nlp-concepts" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<div class="section" id="text-preprocessing">
<h2>Text Preprocessing<a class="headerlink" href="#text-preprocessing" title="Permalink to this headline"></a></h2>
<p><strong>Preprocessing Steps:</strong></p>
<ul class="simple">
<li><p>Remove stopwords and extra whitespaces</p></li>
<li><p>Normalize text to lowercase</p></li>
<li><p>Remove punctuation</p></li>
<li><p><strong>Lemmatization</strong>: converting a word to its base form (e.g. caring to care)</p>
<ul>
<li><p>Lemmatization considers the context and converts the word to its meaningful base for</p></li>
</ul>
</li>
<li><p><strong>Stemming</strong>: removes the last few characters, often leading to incorrect meaning.</p></li>
</ul>
</div>
<div class="section" id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Vocabulary refers to the set of unique tokens in the corpus.</p></li>
<li><p>Vocabulary can be constructed by considering each unique token in the corpus or by considering the top K Frequently Occurring Words.</p>
<ul>
<li><p>vocab = sorted(set(text))</p></li>
<li><p>vocab_to_int = {c: i for i, c in enumerate(vocab)}</p></li>
<li><p>int_to_vocab = dict(enumerate(vocab))</p></li>
<li><p>encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)Text Feature Extraction</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tfidf">
<h2>TFIDF<a class="headerlink" href="#tfidf" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Term Frequency — Inverse Document Frequency”.</p></li>
<li><p>Rare terms are more informative than frequent terms</p></li>
<li><p>We want to give higher weight to rare terms.</p></li>
<li><p>Frequent terms are less informative than rare terms</p></li>
<li><p>We want to give positive weights for frequent terms but lower weights than for rare terms</p></li>
</ul>
<p><strong>Term Frequency:</strong></p>
<ul class="simple">
<li><p>The number of times a word appears in a document divided by the total number of words in the document.</p></li>
</ul>
<p><strong>Inverse Document Frequency</strong></p>
<ul class="simple">
<li><p>The log of the number of documents divided by the number of documents that contain the word “w”.</p></li>
<li><p>The log is used to dampen the inverse document frequency.</p></li>
</ul>
<p><img alt="image" src="../../_images/tfidf.png" /></p>
<p>Where: <code class="docutils literal notranslate"><span class="pre">N</span></code> = number of documents in collection.</p>
<p><strong>Example</strong>
<img alt="image" src="../../_images/tfidf2.png" />
<img alt="image" src="../../_images/tfidf3.png" /></p>
<p><strong>TF-IDF Code Example</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">document1</span><span class="p">,</span> <span class="n">document2</span><span class="p">])</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

<span class="n">dense</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>
<span class="n">denlist</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">denselist</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Reference</strong>
<a class="reference external" href="https://www.youtube.com/watch?v=pxYkuWEOKjc">Christopher Manning Lectures | Stanford University</a></p>
</div>
<div class="section" id="bag-of-words">
<h2>Bag of Words<a class="headerlink" href="#bag-of-words" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>In bag of words representation the length of the vector is determined by the number of unique words in our corpus.
<strong>Disadvantage:</strong></p></li>
<li><p>Lack of meaningful relations and no consideration for order of words: BOW is a collection of words that appear in the text or sentences with the word counts.</p></li>
<li><p>Bag of words does not take into consideration the order in which they appear.</p></li>
<li><p>Sparse Matrix</p></li>
</ul>
</div>
<div class="section" id="skip-grams">
<h2>Skip Grams<a class="headerlink" href="#skip-grams" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Train the neural network to do the following.</p></li>
<li><p>Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random.</p></li>
<li><p>The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p></li>
<li><p>Skip gram predicts the surrounding context words within a specific window given the current word.</p></li>
<li><p>The input layer contains the current word and the output layer contains the
context words.</p></li>
<li><p>The hidden layer contains the number of dimensions in which
we want to represent the current word present at the input layer.</p></li>
</ul>
</div>
<div class="section" id="named-entity-recognition">
<h2>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Named entity recognition (NER) seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.</p></li>
<li><p>The idea of NER is to tag a set of words in a sequence with a label representing the kind of entity the word belongs to.</p></li>
</ul>
<p><strong>Approaches</strong></p>
<ul class="simple">
<li><p>Machine Learning:</p>
<ul>
<li><p>Treat the problem as a multi-class classification where named entities are our labels so we can apply different classification algorithms.</p></li>
<li><p>The problem here is that identifying and labeling named entities require domain understanding of the context of a sentence and sequence of the word labels in it</p></li>
</ul>
</li>
<li><p>Deep Learning:</p>
<ul>
<li><p>A bidirectional LSTM is a combination of two LSTMs — one runs forward from “right to left” and one runs backward from “left to right”</p></li>
<li><p>Embeddings using ELMo: One of the biggest benefits of this approach is that we don’t need any feature engineering; all we need is the sentences and its labeled words, the rest of the work is carried on by ELMo embeddings.</p></li>
</ul>
</li>
</ul>
<p><strong>Reference</strong>:
https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/book/nlp/concepts.md.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2021, Trace Smith, Damon Resnick.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>