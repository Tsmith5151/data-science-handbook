<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformers &mdash; Data Science Hand.. 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Data Science Hand..
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Transformers</a><ul>
<li><a class="reference internal" href="#self-attention">Self-Attention</a><ul>
<li><a class="reference internal" href="#why-transformers">Why Transformers?</a></li>
<li><a class="reference internal" href="#encoders">Encoders</a></li>
<li><a class="reference internal" href="#decoders">Decoders</a></li>
<li><a class="reference internal" href="#transformer-architecture">Transformer Architecture</a></li>
<li><a class="reference internal" href="#calculate-self-attention">Calculate Self-Attention</a></li>
</ul>
</li>
<li><a class="reference internal" href="#bert">BERT</a><ul>
<li><a class="reference internal" href="#bert-tasks">BERT Tasks</a></li>
<li><a class="reference internal" href="#why-bert">Why Bert?</a></li>
<li><a class="reference internal" href="#bert-overview">BERT Overview</a></li>
<li><a class="reference internal" href="#inner-workings-of-bert">Inner-workings of BERT</a></li>
<li><a class="reference internal" href="#bert-tensorflow">BERT - Tensorflow</a></li>
<li><a class="reference internal" href="#bert-semantics-similarity">BERT - Semantics Similarity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Data Science Hand..</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Transformers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/book/nlp/transformers.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<div class="section" id="self-attention">
<h2>Self-Attention<a class="headerlink" href="#self-attention" title="Permalink to this headline"></a></h2>
<div class="section" id="why-transformers">
<h3>Why Transformers?<a class="headerlink" href="#why-transformers" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>paper: <em>Attention Is All You Need - 2017</em></p></li>
<li><p>Attention allows the model to focus on the relevant parts of the input
sequence as needed.</p></li>
<li><p>Self-attention is the method the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> uses to bake the
“understanding” of other  relevant words into the one we’re currently
processing.</p></li>
<li><p>Models like <code class="docutils literal notranslate"><span class="pre">ELMo</span></code> which uses LSTMs to alleviate the consequences of not
having an attention mechanism to create an efficient way of <code class="docutils literal notranslate"><span class="pre">focusing</span></code> on the
important word in each sentence.</p></li>
<li><p>This is the problem the Transformer network addressed by using the attention mechanism.</p></li>
</ul>
<p><strong>Overview</strong></p>
<ul class="simple">
<li><p>Transformers compute <code class="docutils literal notranslate"><span class="pre">vector-space</span> <span class="pre">representations</span></code> of natural language that
are suitable for use in deep learning models.</p></li>
<li><p>Based solely on attention mechanisms to compute representations of its input
and output without using sequence aligned RNNs or convolutions.</p></li>
<li><p>The benefit of the transformer architecture is that it helps the model to retain infinitely long sequences that were not possible from the traditional RNNs, LSTMs, and GRU</p></li>
<li><p>Still lacks contextual understanding.</p></li>
</ul>
<p><strong>Structure</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Attention</span> <span class="pre">is</span> <span class="pre">all</span> <span class="pre">you</span> <span class="pre">need</span></code> paper used attention to improve the performance
of machine translation. They created a model with two main parts:</p>
<ul>
<li><p><strong>Encoder</strong>: This part of the “Attention is all you need” model processes
the input text, looks for important parts, and creates an embedding for
each word based on relevance to other words in the sentence. (Stack of 6
encoders)</p></li>
<li><p><strong>Decoder</strong>: This takes the output of the encoder, which is an embedding,
and then turns that embedding back into a text output, i.e. the translated
version of the input text (Stack of 6 Decoders).</p></li>
</ul>
</li>
</ul>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>Neither the encoder nor the decoder used any recurrence or looping, like traditional RNNs.</p></li>
<li><p>Instead, they used layers of <code class="docutils literal notranslate"><span class="pre">attention</span></code> through which the information passes
linearly. It didn’t loop over the input multiple times – instead, the
Transformer passes the input through multiple attention layers.</p></li>
<li><p>You can think of each attention layer as <code class="docutils literal notranslate"><span class="pre">learning</span></code> more about the input,
i.e. looking at different parts of the sentence and trying to discover more
semantic or syntactic information.</p></li>
<li><p>This is important in the context of the vanishing gradient problem.</p></li>
<li><p><strong>Reference</strong>: http://jalammar.github.io/illustrated-bert/</p></li>
</ul>
</div>
<div class="section" id="encoders">
<h3>Encoders<a class="headerlink" href="#encoders" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The encoders are all identical in structure (yet they do not share weights).
Each one is broken down into two sublayers: self-attention and feed-forward</p></li>
<li><p>The encoder’s inputs first flow through a self-attention layer which helps
the encoder look at other words in the input sentence as it encodes a
specific word.</p></li>
<li><p>The outputs of the self-attention layer are fed to a feed-forward neural network.</p></li>
</ul>
<p><img alt="image" src="../../_images/encoder.png" /></p>
</div>
<div class="section" id="decoders">
<h3>Decoders<a class="headerlink" href="#decoders" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The decoder has both those layers, but between them is an attention layer
that helps the decoder focus on relevant parts of the input sentence</p></li>
<li><p>The output of the top encoder is then transformed into a set of attention vectors K and V.</p></li>
<li><p>These are to be used by each decoder in its “encoder-decoder attention” layer
which helps the decoder focus on appropriate places in the input sequence</p></li>
<li><p>The decoder stack outputs a vector of floats which is passed to a final fully connected layer that projects the vector produced by the stack of decoders into a logits vector</p></li>
<li><p>Softmax layer then turns those scores into probabilities</p></li>
<li><p>Output: the cell with the highest probability is chosen, and the word
associated with it is produced as the output for this time step.</p></li>
<li><p><strong>Reference</strong>: http://jalammar.github.io/illustrated-transformer/</p></li>
</ul>
<p><img alt="image" src="../../_images/decoder.png" /></p>
</div>
<div class="section" id="transformer-architecture">
<h3>Transformer Architecture<a class="headerlink" href="#transformer-architecture" title="Permalink to this headline"></a></h3>
<p><img alt="image" src="../../_images/bert1.png" /></p>
</div>
<div class="section" id="calculate-self-attention">
<h3>Calculate Self-Attention<a class="headerlink" href="#calculate-self-attention" title="Permalink to this headline"></a></h3>
<ol class="simple">
<li><p>For each word, create a Query vector, a Key vector, and a Value vector and
are created by multiplying the embedding by three matrices that we trained
during the training process</p></li>
<li><p>Calculate a score to determine how much focus to place on other parts of the
input sentence as we encode a word at a certain position.<br />The score is calculated by taking the dot product of the query vector with
the key vector<br />So if we’re processing the self-attention for the word in position #1, the
first score would be the dot product of q1 and k1.<br />The second score would be the dot product of q1 and k2 and so on</p></li>
<li><p>Divide scores by 8; having more stable gradients (paper)</p></li>
<li><p>Apply softmax to normalize the scores so they’re all positive and add up to 1.
Determines how much each word will be expressed at this position (e.g. first
word, etc.)</p></li>
<li><p>Multiply each value vector by the softmax score; this helps drown-out
irrelevant words when multiplying by small values.</p></li>
<li><p>Sum up the weighted value vectors, which produces the output of the
self-attention layer</p></li>
<li><p>Resulting vector is one we can send along to the feed-forward neural
network.</p></li>
</ol>
</div>
</div>
<hr class="docutils" />
<div class="section" id="bert">
<h2>BERT<a class="headerlink" href="#bert" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The BERT family of models uses the <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> encoder architecture to
process each token of  input text in the full context of all tokens before
and after, hence the name: <code class="docutils literal notranslate"><span class="pre">Bidirectional</span> <span class="pre">Encoder</span> <span class="pre">Representations</span> <span class="pre">from</span> <span class="pre">Transformers</span></code>.</p></li>
<li><p>BERT models are usually pre-trained on a large corpus of text, then
fine-tuned for specific tasks.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert">Reference</a></p></li>
</ul>
<div class="section" id="bert-tasks">
<h3>BERT Tasks<a class="headerlink" href="#bert-tasks" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Question answering</p></li>
<li><p>NER</p></li>
<li><p>Semantic similarity</p></li>
<li><p>Document classification</p></li>
<li><p>Predicting next word</p></li>
</ul>
</div>
<div class="section" id="why-bert">
<h3>Why Bert?<a class="headerlink" href="#why-bert" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>There were still issues with the limits of training large amounts of data
using approaches like ELMo and Word2Vec.</p></li>
<li><p>This was a serious obstacle to the potential of these models to improve their
ability to  perform well on a range of NLP tasks. This is where the concept
of pre-training set the scene for the arrival of models like BERT to
accelerate the evolution</p></li>
</ul>
</div>
<div class="section" id="bert-overview">
<h3>BERT Overview<a class="headerlink" href="#bert-overview" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>BERT is pre-trained on a large corpus of unlabelled text including the entire
Wikipedia and BookCorpus with <code class="docutils literal notranslate"><span class="pre">30MM</span></code> tokens.</p></li>
<li><p>Used for transfer learning</p></li>
<li><p>BERT makes use of <code class="docutils literal notranslate"><span class="pre">Masked</span> <span class="pre">Language</span> <span class="pre">Models</span></code> to randomly mask words in the
sentence and then it tries to predict them.</p></li>
<li><p>Masking is where the model looks in both directions and it uses the full
context of the sentence in order to predict the masked word.
The bidirectional aspect creates a representation of each word that is based
on the other words in the sentence - context-based.</p></li>
<li><p><strong>Example:</strong></p>
<ul>
<li><p>“I accessed the bank account,” a unidirectional contextual model would
represent “bank” based on “I accessed the” but not “account.”</p></li>
<li><p>BERT relies on Transformers (the attention mechanism that learns contextual
relationships between words in a text).</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="inner-workings-of-bert">
<h3>Inner-workings of BERT<a class="headerlink" href="#inner-workings-of-bert" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The input to the encoder for BERT is a sequence of tokens, which are first
converted into vectors and then processed in the neural network.</p></li>
<li><p><strong>Token embeddings:</strong> A <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token is added to the input word tokens at the
beginning of the first sentence and a <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> token is inserted at the end of
each sentence.</p></li>
<li><p><strong>Segment embeddings:</strong> A marker indicating Sentence A or Sentence B is added
to each token. This allows the encoder to distinguish between sentences.</p></li>
<li><p><strong>Positional embeddings:</strong> A positional embedding is added to each token to
indicate its position in the sentence.</p>
<ul>
<li><p><strong>BERT-Base:</strong> 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters</p></li>
<li><p><strong>BERT-Large:</strong> 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="bert-tensorflow">
<h3>BERT - Tensorflow<a class="headerlink" href="#bert-tensorflow" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Load a pre-trained BERT model from <code class="docutils literal notranslate"><span class="pre">TensorFlow</span> <span class="pre">Hub</span></code>.</p></li>
<li><p><strong><a class="reference external" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3">BERT-Base, Uncased</a></strong></p>
<ul>
<li><p>Text inputs have been normalized the “uncased” way, meaning that the text
has been lower-cased before tokenization into word pieces, and any accent
markers have been stripped.</p></li>
</ul>
</li>
<li><p><strong><a class="reference external" href="https://tfhub.dev/google/collections/bert/1">Small BERTs</a></strong></p>
<ul>
<li><p>Small BERTs have the same general architecture but fewer and/or smaller
Transformer blocks, which lets you explore tradeoffs between speed, size
and quality.</p></li>
</ul>
</li>
<li><p>BertTokenizer
Tokenizer classes which store the vocabulary for each model and provide
methods for encoding/decoding strings in list of token embeddings indices</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_word_ids</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_mask</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_type_ids</span></code></p></li>
</ul>
</div>
<div class="section" id="bert-semantics-similarity">
<h3>BERT - Semantics Similarity<a class="headerlink" href="#bert-semantics-similarity" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Initial problem with BERT for semantic similarities:</p></li>
<li><p>Finding the most similar pair in a collection of 10,000 sentences requires
about 50 million inference computations (~65 hours) with BERT.</p></li>
<li><p>The construction of BERT makes it unsuitable for semantic similarity search</p></li>
<li><p>New research came out in 2019 with a modification to BERT to derive
semantically meaningful sentence embeddings that can be compared using
<code class="docutils literal notranslate"><span class="pre">cosine-similarity</span></code>.</p></li>
<li><p>This approach allows us to use contextualized embeddings and use <code class="docutils literal notranslate"><span class="pre">SBERT</span></code></p></li>
<li><p>We can use the <code class="docutils literal notranslate"><span class="pre">SentenceTransformer</span></code> model in the Hugging Face library to map
sentences to embeddings.</p></li>
<li><p>BERT is limited to <code class="docutils literal notranslate"><span class="pre">512</span> <span class="pre">word</span> <span class="pre">pieces</span></code>, which corresponds to about 300-400 words</p></li>
<li><p>If resumes and job descriptions exceed this constraint, we would need to
think about ways to work around this.</p></li>
<li><p>One approach would be to break the resume/job description up into smaller
sections and compute a similarity score for each section then take the
average. We would then rank the top <code class="docutils literal notranslate"><span class="pre">n</span></code> jobs to a user based on this
information.</p></li>
<li><p><strong>Link</strong>: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial</p></li>
</ul>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>