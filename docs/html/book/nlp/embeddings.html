
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Word Embeddings &#8212; Data Science Hand.. 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/style.css" />
    <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.12.4.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.4.1/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-inverse navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          Handbook</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Section <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Word Embeddings</a></li>
<li><a class="reference internal" href="#word2vec">Word2Vec</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="word-embeddings">
<h1>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<ul class="simple">
<li><p>Mechanism to express relationships in relative vector distance.</p></li>
<li><p>Method where words are encoded as real-valued vectors in a high dimensional
space, where the similarity between words in terms of meaning translates to
closeness in the vector space.
The vector representations capture quite a bit of the information, meaning,
and associations of words:</p></li>
<li><p>In other words, emeddings represent words in a way that captures semantic.
relationships -  the ability to tell if words are similar or not.</p></li>
<li><p>From the word embeddings model, similar words would be in close proximity.</p></li>
</ul>
<p><img alt="image" src="../../_images/embeddings.png" />
<a class="reference external" href="https://www.youtube.com/watch?v=LE3NfEULV6k">Reference</a></p>
<p><strong>Why Embeddings</strong></p>
<ul class="simple">
<li><p>When you’re dealing with words in text, you end up with tens of thousands of
classes to predict, one for each word.</p></li>
<li><p>Trying to one-hot encode these words is massively inefficient; you’ll have a
sparse vector representation where an element is set to 1 and the other
50,000+ set to 0.</p></li>
<li><p>This ends up causing the matrix multiplication in the first hidden layer to
have almost all  of the resulting values be zero and is a huge waste of
computation.</p></li>
<li><p>To solve this problem and greatly increase the efficiency of our networks, we
can use embeddings.</p></li>
</ul>
<p><strong>How do embedding work?</strong></p>
<ul class="simple">
<li><p>We basically train a simple neural network with a single hidden layer with
the goal to learn the weights of the hidden layer, which are the work
vectors.</p></li>
<li><p>We cannot just feed a word just as a text string to a neural network, so we
instead create a one-hot encoded vector for each of these words in the
vocabulary; which is the length of our one-hot encoded vector.</p></li>
<li><p>The hidden layer is going to be represented by a weight matrix with one for
every word in our vocabulary and one for every hidden neuron.</p></li>
<li><p>The output of the hidden layer is just the “word vector” for the input word.</p></li>
<li><p>The output of the network is a single vector containing, for every word in
our vocabulary, compute the softmax to obtain the probability that a randomly
selected nearby word is that vocabulary word.</p></li>
<li><p>Can compute the cosine similarity of the angle between such vectors should be
close to 1, angle between vectors close to 0).</p></li>
</ul>
<p><strong><span class="label label-warning">Limitations</span></strong></p>
<ul class="simple">
<li><p>Impossible to know word representation in the context of the full sentence.</p></li>
<li><p>Example: the word “present” - can mean gift or time, depending on the context.</p></li>
</ul>
<p><img alt="image" src="../../_images/embeddings2.png" /></p>
</div>
<div class="section" id="word2vec">
<h1>Word2Vec<a class="headerlink" href="#word2vec" title="Permalink to this headline"></a></h1>
<ul class="simple">
<li><p><strong>Summary</strong>: In this technique, we map each word of a sentence to an
embedding vector which tends to be smaller than a bag of word representation.</p></li>
<li><p>Word2Vec embeddings are pre-trained embeddings that have been determined in an
unsupervised manner.</p>
<ul>
<li><p>These vectors have a really nice property, similar context words tend to
have vectors that are collinear; point to roughly the same direction.</p></li>
<li><p>Example: Words that show up in similar contexts, such as:</p>
<ul>
<li><p>“blue”, “red”, and “white” will have vectors near each other.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Pre-trained Model:</strong></p>
<ul>
<li><p>Data size 1.5GB</p></li>
<li><p>It includes word vectors for a vocabulary of 3 million words phrases that
they trained on roughly 100 billion words from a Google News dataset.</p></li>
</ul>
</li>
</ul>
<p><strong><span class="label label-warning">Problems</span></strong></p>
<ul class="simple">
<li><p>Perhaps the biggest problem with word2vec is the inability to handle unknown
or out-of-vocabulary (OOV) words. If your model hasn’t encountered a word
before, it will have no idea how to interpret it or how to build a vector for
it.</p></li>
</ul>
<p><strong>How it works</strong></p>
<ul class="simple">
<li><p>Before the training process starts, we pre-process the text we’re training
the model against. In this step, we determine the size of our vocabulary and
which words belong to it.</p></li>
<li><p>At the start of the training phase, we create two matrices – an Embedding
matrix and a Context matrix.</p></li>
<li><p>These two matrices have an embedding for each word in our vocabulary (So
vocab_size is one of their dimensions). The second dimension is the number of
units in the hidden layer, which is the embedding_size – 300 is a common
value.</p></li>
<li><p>At the start of the training process, we initialize these matrices with
random values. Then we start the training process. In each training step, we
take one positive example and its associated negative examples.</p></li>
<li><p>We proceed to look up their embeddings – for the input word, we look in the
Embedding matrix. For the context words, we look in the Context matrix.
we take the dot product of the input embedding with each of the context
embeddings. In each case, that would result in a number, that number
indicates the similarity of the input and context embeddings. To turn into
probabilities, we can take the sigmoid.</p></li>
<li><p>Now that the untrained model has made a prediction, and seeing as though we
have an actual target label to compare against, let’s calculate how much
error is in the model’s prediction. To do that, we just subtract the sigmoid
scores from the target labels. And then we perform back propagation to update
the weights.</p></li>
</ul>
<p><img alt="image" src="../../_images/word2vec.png" />
<a class="reference external" href="https://www.youtube.com/watch?v=LE3NfEULV6k">Reference</a></p>
<p><strong>Code Example</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">min_count</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">window</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;alice&#39;</span><span class="p">,</span> <span class="s1">&#39;machines&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../../_sources/book/nlp/embeddings.md.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2021, Trace Smith, Damon Resnick.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>