<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Recurrent Neural Networks &mdash; Data Science Hand.. 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Data Science Hand..
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Recurrent Neural Networks</a><ul>
<li><a class="reference internal" href="#gru">GRU</a></li>
<li><a class="reference internal" href="#lstm">LSTM</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Data Science Hand..</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Recurrent Neural Networks</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/book/deep-learning/rnn.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="recurrent-neural-networks">
<h1>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Feed-forward</span> <span class="pre">network</span></code> that is rolled out over time and operates over
sequences. State receives input  vector from the previous layer (or step) and
can modify that state at what it receives at every step. With RNNs, we use
<code class="docutils literal notranslate"><span class="pre">Backpropagation</span></code> in time.</p></li>
</ul>
<p><img alt="image" src="../../_images/rnn.png" /></p>
<p><strong>Problem with RNNs:</strong></p>
<ul class="simple">
<li><p>You are repeatedly multiplying by the same weight matrix.</p></li>
<li><p>This can cause <code class="docutils literal notranslate"><span class="pre">exploding</span></code> or <code class="docutils literal notranslate"><span class="pre">vanishing</span> <span class="pre">gradients</span></code> – where the model is not
capable learning long sequences.</p></li>
<li><p><strong>Exploding gradients</strong> are less of the problem since you could easily apply a
simple gradient clipping algorithm.</p></li>
<li><p><strong>Vanishing gradients</strong> can also be difficult to spot making it more dangerous
when deploying your system into production.</p></li>
</ul>
<p><strong>Method:</strong></p>
<ul class="simple">
<li><p>First, the initial hidden state, which is typically a vector of zeros and the
hidden state weight is multiplied and then the hidden state bias is added to
the result.</p></li>
<li><p>In the meantime, the input at the time step “t” and the input weight is
multiplied and the input bias is added to that result.</p></li>
<li><p>We can obtain the hidden state at time step t by sending the addition of the
above two results through an activation function, typically <code class="docutils literal notranslate"><span class="pre">tanh</span></code>.</p></li>
</ul>
<div class="section" id="gru">
<h2>GRU<a class="headerlink" href="#gru" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>With RNNs, long products of matrices can lead to vanishing or divergent gradients.</p></li>
<li><p>We may have cases where an early observation is highly significant for
predicting all future observations so we would like to have some mechanisms
for storing vital early information in a <strong>memory cell</strong>.</p></li>
</ul>
<p><strong>The key distinction between regular RNNs and GRUs:</strong></p>
<ul class="simple">
<li><p>The latter support gating of the hidden state. This means that we have
dedicated mechanisms for when  a hidden state should be updated and also when
it should be reset.</p></li>
<li><p><strong>Components:</strong></p>
<ul>
<li><p><strong>Reset gate:</strong> helps capture short-term dependencies</p></li>
<li><p><strong>Update gate:</strong> help capture long-term dependencies</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>To address long-term information preservation by a memory cell with a series
of gates to control information flow.</p></li>
<li><p><strong>The LSTM gates are:</strong></p>
<ul>
<li><p><strong>Input Gate:</strong> To update the cell state, we have the input gate. We first
pass the previous  hidden state and current input into a sigmoid function.
That decides which values will be updated by transforming the values to be
between 0 and 1. 0 means not important, and 1 means important</p></li>
<li><p><strong>Forget Gate:</strong> decides what information should be thrown away or kept.
Information from the previous hidden state and information from the
current input is passed through the sigmoid function. Values come out
between 0 and 1. The closer to 0 means to forget, and the closer to 1 means
to keep.</p></li>
<li><p><strong>Output Gate:</strong> decides what the next hidden state should be. Remember
that the hidden state contains information on previous inputs.</p></li>
</ul>
</li>
</ul>
<p><img alt="image" src="../../_images/lstm.png" /></p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>