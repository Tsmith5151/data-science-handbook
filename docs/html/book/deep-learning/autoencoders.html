<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Autoencoders &mdash; Data Science Hand.. 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Data Science Hand..
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Autoencoders</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#lstm-autoencoder">LSTM Autoencoder</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Data Science Hand..</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Autoencoders</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/book/deep-learning/autoencoders.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="autoencoders">
<h1>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline"></a></h1>
<hr class="docutils" />
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">autoencoder</span></code> is a machine learning solution made up of two blocks,
encoder and decoder, whose  purpose is to obtain internal representations
usually with smaller dimensionality than the input. This process is known as
encoding.</p></li>
<li><p>For this representation to be obtained, the decoding phase is also necessary
so that the system can  encode efficiently the input data.</p></li>
<li><p>The purpose of this block of the autoencoder is to reconstruct the input
signal from the intermediate representation obtained by the encoder.</p></li>
<li><p>The difference between the reconstructed signal by the autoencoder and the
original input signal is  known as the reconstruction error.</p></li>
<li><p>In essence, the autoencoder tries to learn an identity function which makes
the output  be similar  to the input x.</p></li>
<li><p>By placing constraints on the network, such as a limitation in the number of
hidden units, interesting structure about the data can be discovered.</p></li>
<li><p><strong>Example</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Convolutional</span> <span class="pre">Autoencoders</span></code> are designed to encode the input into a set of
simpler signals and reconstruct the input from them.</p></li>
<li><p>The encoder layers are in this case convolutional layers and the decoder
layers are called <code class="docutils literal notranslate"><span class="pre">deconvolution</span></code> or <code class="docutils literal notranslate"><span class="pre">upsampling</span></code> layers.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="lstm-autoencoder">
<h2>LSTM Autoencoder<a class="headerlink" href="#lstm-autoencoder" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Recurrent neural networks, such as the Long Short-Term Memory, or LSTM,
network are specifically designed to support sequences of input data.</p></li>
<li><p>They are capable of learning the complex dynamics within the temporal
ordering of input sequences as  well as use an internal memory to remember or
use information across long input sequences.</p></li>
<li><p>The LSTM network can be organized into an architecture called the
Encoder-Decoder LSTM that allows  the model to be used to both support
variable length input sequences and to predict or output variable length
output sequences.</p></li>
<li><p>This architecture is the basis for many advances in complex sequence
prediction problems such as  speech recognition and text translation.</p></li>
<li><p>In this architecture, an encoder LSTM model reads the input sequence
step-by-step. After reading in  the entire input sequence, the hidden state
or output of this model represents an internal learned representation of the
entire input sequence as a fixed-length vector. This vector is then provided
as an input to the decoder model that interprets it as each step in the
output sequence is generated.</p></li>
<li><p>An <code class="docutils literal notranslate"><span class="pre">LSTM</span> <span class="pre">Autoencoder</span> </code>is an implementation of an autoencoder for sequence
data using an Encoder-Decoder LSTM architecture.</p></li>
<li><p>For a given dataset of sequences, an encoder-decoder LSTM is configured to
read the input sequence,  encode it, decode it, and recreate it.</p></li>
<li><p>The performance of the model is evaluated based on the model’s ability to
recreate the input  sequence.</p></li>
<li><p>Once the model achieves a desired level of performance recreating the
sequence, the decoder part of  the model may be removed, leaving just the
encoder model.</p></li>
<li><p>This model can then be used to encode input sequences to a fixed-length vector.</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>