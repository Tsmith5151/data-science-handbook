<!DOCTYPE html>
<html class="writer-html5" lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Data Preprocessing &mdash; Data Science Hand.. 0.0.1 documentation</title>
  <link rel="stylesheet" href="../../assets/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../assets/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.min.js"></script>
  <![endif]-->

  <script data-url_root="../" id="documentation_options" src="../../assets/documentation_options.js"></script>
  <script src="../../assets/jquery.js"></script>
  <script src="../../assets/underscore.js"></script>
  <script src="../../assets/doctools.js"></script>
  <script src="../../assets/js/theme.js"></script>
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Data Science Hand..
          </a>
          <div role="search">
            <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
              <input type="text" name="q" placeholder="Search docs" />
              <input type="hidden" name="check_keywords" value="yes" />
              <input type="hidden" name="area" value="default" />
            </form>
          </div>
        </div>
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
          <!-- Local TOC -->
          <div class="local-toc">
            <ul>
              <li><a class="reference internal" href="#">Data Preprocessing</a></li>
              <li><a class="reference internal" href="#missing-values">Missing Values</a></li>
              <li><a class="reference internal" href="#outliers">Outliers</a></li>
              <li><a class="reference internal" href="#categorical-encoding">Categorical Encoding</a></li>
              <li><a class="reference internal" href="#data-normalization">Data Normalization</a></li>
              <li><a class="reference internal" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
            </ul>
          </div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" aria-label="Mobile navigation menu">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Data Science Hand..</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
            <ul class="wy-breadcrumbs">
              <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
              <li>Data Preprocessing</li>
              <li class="wy-breadcrumbs-aside">
                <a href="../_sources/../dataprep.md.txt" rel="nofollow"> View page source</a>
              </li>
            </ul>
            <hr />
          </div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div itemprop="articleBody">

              <hr class="docutils" />
              <section id="data-preprocessing">
                <h1>Data Preprocessing<a class="headerlink" href="#data-preprocessing"
                    title="Permalink to this headline"></a></h1>
              </section>
              <hr class="docutils" />
              <section id="missing-values">
                <h1>Missing Values<a class="headerlink" href="#missing-values" title="Permalink to this headline"></a>
                </h1>
                <ul class="simple">
                  <li>
                    <p>Reasons:</p>
                    <ul>
                      <li>
                        <p>Missing completely at random (MCAR)</p>
                      </li>
                      <li>
                        <p>Missing at random (MAR)</p>
                      </li>
                      <li>
                        <p>Not missing at random (NMAR)</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p>How to handle the missing values:</p>
                    <ul>
                      <li>
                        <p>Do Nothing:</p>
                        <ul>
                          <li>
                            <p>Models like XGBoost can deal with missing values by deciding for each sample which is the
                              best way to impute them and learns the best values</p>
                          </li>
                        </ul>
                      </li>
                      <li>
                        <p>Imputation:</p>
                        <ul>
                          <li>
                            <p>Using (Mean/Median) Value</p>
                          </li>
                          <li>
                            <p>Using (Most Frequent) Value</p>
                          </li>
                          <li>
                            <p>Using k-NN</p>
                          </li>
                          <li>
                            <p>Interpolation (Linear/Nearest Neighbors)</p>
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </section>
              <section id="outliers">
                <h1>Outliers<a class="headerlink" href="#outliers" title="Permalink to this headline"></a></h1>
                <p><strong>Identify Outliers</strong></p>
                <ul class="simple">
                  <li>
                    <p><strong>Cook’s Distance:</strong></p>
                    <ul>
                      <li>
                        <p>Measures the effect of deleting a given observation. It represents the sum of all the changes
                          in the regression model when observation “i” is removed from it.</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p><strong>Interquartile Range Method (IQR):</strong></p>
                    <ul>
                      <li>
                        <p>Is a good statistic for summarizing a non-Gaussian distribution sample of data.</p>
                      </li>
                      <li>
                        <p>IQR is calculated as the difference between the 75th and the 25th percentiles of the data and
                          defines the box in a box and whisker plot.</p>
                      </li>
                      <li>
                        <p>The IQR defines the middle 50% of the data, or the body of the data</p>
                      </li>
                      <li>
                        <p>Can be used to identify outliers by defining limits on the sample values that are below the
                          25th percentile or above the 75th percentile.</p>
                      </li>
                      <li>
                        <p>Linear Models: Projection methods that model the data into lower dimensions using linear
                          correlations.</p>
                      </li>
                      <li>
                        <p>For example, PCA and data with large residual errors may be outliers.</p>
                      </li>
                      <li>
                        <p>Proximity-based Models: Data instances that are isolated from the mass of the data as
                          determined by cluster, density or KNN analysis.</p>
                      </li>
                    </ul>
                  </li>
                </ul>
                <p><strong>Handling Outliers</strong></p>
                <ul class="simple">
                  <li>
                    <p><strong>Log-Scale Transformation:</strong> This method is often used to reduce the variability of
                      data including outlying observation.</p>
                  </li>
                  <li>
                    <p><strong>Model Selection:</strong> Tree based models are less impacted by outliers compared to
                      linear models.</p>
                    <ul>
                      <li>
                        <p>XGBoost and boosting in general are very sensitive to outliers.</p>
                      </li>
                      <li>
                        <p>This is because boosting builds each tree on previous trees’ residuals/errors.</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p>Outliers will have much larger residuals than non-outliers, so boosting will focus a
                      disproportionate amount of its attention on those points</p>
                  </li>
                </ul>
              </section>
              <section id="categorical-encoding">
                <h1>Categorical Encoding<a class="headerlink" href="#categorical-encoding"
                    title="Permalink to this headline"></a></h1>
                <p><strong>One Hot Encoding:</strong></p>
                <ul class="simple">
                  <li>
                    <p>Maps each category to a vector that contains 1 and 0 denoting the presence or absence of the
                      feature.</p>
                  </li>
                  <li>
                    <p>The number of vectors depends on the number of categories for features.</p>
                  </li>
                  <li>
                    <p>This method produces a lot of columns that slows down the learning significantly if the number of
                      the category is very high for the feature.</p>
                  </li>
                </ul>
                <p><strong>Label Encoder:</strong></p>
                <ul class="simple">
                  <li>
                    <p>Each category is assigned a value from 1 through N (here N is the number of categories for the
                      feature.</p>
                  </li>
                  <li>
                    <p>One major issue with this approach is there is no relation or order between these classes, but
                      the algorithm might consider them as some order, or there is some relationship.</p>
                  </li>
                </ul>
                <p><strong>Ordinal encoding:</strong></p>
                <ul class="simple">
                  <li>
                    <p>To ensure the encoding of variables retains the ordinal nature of the variable.</p>
                  </li>
                  <li>
                    <p>This is reasonable only for ordinal variables.</p>
                  </li>
                  <li>
                    <p>The transformation looks almost similar to Label Encoding but slightly different as Label coding
                      would not consider whether a variable is ordinal or not and it will assign a sequence of integers.
                    </p>
                  </li>
                </ul>
                <p><strong>Binary Encoding:</strong></p>
                <ul class="simple">
                  <li>
                    <p>Converts a category into binary digits.</p>
                  </li>
                  <li>
                    <p>Each binary digit creates one feature column.</p>
                  </li>
                  <li>
                    <p>If there are n unique categories, then binary encoding results in the only log(base 2)ⁿ features.
                    </p>
                  </li>
                  <li>
                    <p>Compared to One Hot Encoding, this will require fewer feature columns.</p>
                    <ul>
                      <li>
                        <p><strong>Explain:</strong> for 100 categories One Hot Encoding will have 100 features
                          while forBinary encoding, we will need just seven features.</p>
                      </li>
                    </ul>
                  </li>
                </ul>
              </section>
              <section id="data-normalization">
                <h1>Data Normalization<a class="headerlink" href="#data-normalization"
                    title="Permalink to this headline"></a></h1>
                <ul class="simple">
                  <li>
                    <p>Standardize: scaling features by removing the mean and scaling to unit
                      variance</p>
                  </li>
                  <li>
                    <p>MinMax: Transform features by scaling each feature to a given range [-1,1].</p>
                  </li>
                </ul>
              </section>
              <section id="dimensionality-reduction">
                <h1>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction"
                    title="Permalink to this headline"></a></h1>
                <p><strong>Principal Component Analysis</strong></p>
                <ul class="simple">
                  <li>
                    <p>Dimension reduction technique that finds the variance maximizing directions onto which to project
                      the data.</p>
                  </li>
                  <li>
                    <p>Algorithm to reduce the dimensionality of the data by compressing it onto a new feature subspace,
                      where a subset of the principal components (i.e. eigenvectors) accounts for the highest variance
                      and explains the underlying structure of the overall dataset.</p>
                  </li>
                  <li>
                    <p>The eigenvectors of the correlation or covariance matrix represent the principal components
                      (directions of maximum variance and determine the direction of the new feature space) and the
                      eigenvalues (scalar) correspond to the magnitude of the eigenvectors.</p>
                  </li>
                  <li>
                    <p>The eigenvector with the largest eigenvalue is the direction along which the data set has the
                      maximum variance.</p>
                  </li>
                  <li>
                    <p>After applying the linear PCA transformation, we have a lower dimensional subspace where the
                      samples are “most spread” along the new feature axes.</p>
                  </li>
                  <li>
                    <p>PCA reduces high dimensional space down to two or three principal components without losing much
                      information.</p>
                  </li>
                </ul>
              </section>


            </div>
          </div>
          <footer>

            <hr />

            <div role="contentinfo">
              <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
            </div>

            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
            <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
            provided by <a href="https://readthedocs.org">Read the Docs</a>.


          </footer>
        </div>
      </div>
    </section>
  </div>
  <script>
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(false);
    });
  </script>

</body>

</html>