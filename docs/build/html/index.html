<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Data Science Handbook &mdash; Data Science Handbook 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> Data Science Handbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Data Science Handbook</a><ul>
<li><a class="reference internal" href="#statistics">Statistics</a></li>
<li><a class="reference internal" href="#p-value">P-Value</a></li>
<li><a class="reference internal" href="#z-test">Z-Test</a></li>
<li><a class="reference internal" href="#t-test">T-Test</a></li>
<li><a class="reference internal" href="#statistical-power">Statistical Power</a></li>
<li><a class="reference internal" href="#anova">ANOVA</a></li>
<li><a class="reference internal" href="#f-test">F-Test</a></li>
<li><a class="reference internal" href="#central-limit-theorem">Central Limit Theorem</a></li>
<li><a class="reference internal" href="#confidence-interval">Confidence Interval</a></li>
<li><a class="reference internal" href="#multicollinearity">Multicollinearity</a></li>
<li><a class="reference internal" href="#model-evaluation">Model Evaluation</a></li>
<li><a class="reference internal" href="#regression">Regression</a><ul>
<li><a class="reference internal" href="#r-squared">R-Squared</a></li>
<li><a class="reference internal" href="#adjusted-r-squared">Adjusted R-Squared</a></li>
<li><a class="reference internal" href="#root-mean-squared-error">Root Mean Squared Error</a></li>
<li><a class="reference internal" href="#mean-absolute-error">Mean Absolute Error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">Classification</a><ul>
<li><a class="reference internal" href="#accuracy">Accuracy</a></li>
<li><a class="reference internal" href="#precision">Precision</a></li>
<li><a class="reference internal" href="#recall">Recall</a></li>
<li><a class="reference internal" href="#f1-score">F1 Score</a></li>
<li><a class="reference internal" href="#specificity">Specificity</a></li>
<li><a class="reference internal" href="#false-positives">False positives</a></li>
<li><a class="reference internal" href="#false-negatives">False negatives</a></li>
<li><a class="reference internal" href="#auc-score">AUC Score</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Data Science Handbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Data Science Handbook</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="data-science-handbook">
<h1>Data Science Handbook<a class="headerlink" href="#data-science-handbook" title="Permalink to this headline"></a></h1>
<div class="toctree-wrapper compound">
</div>
<hr class="docutils" />
<section id="statistics">
<h2>Statistics<a class="headerlink" href="#statistics" title="Permalink to this headline"></a></h2>
</section>
<hr class="docutils" />
<section id="p-value">
<h2>P-Value<a class="headerlink" href="#p-value" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Statistically significant is detected when the observed p-value from the test statistic is less than the level of significance (i.e. alpha).</p></li>
<li><p>Meaning, there is a very low probability of observing by random chance something as extreme or more extreme than what was observed under the assumption that the null hypothesis is true.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="o">-</span><span class="n">Value</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="o">=</span> <span class="n">Reject</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="p">;</span>
<span class="n">P</span><span class="o">-</span><span class="n">Value</span> <span class="o">&gt;</span> <span class="mf">0.05</span> <span class="o">=</span> <span class="n">Fail</span> <span class="n">to</span> <span class="n">reject</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="p">;</span>
</pre></div>
</div>
<p><strong>Note:</strong> If we reject the null hypothesis, this indicates strong statistical significance.</p>
</section>
<section id="z-test">
<h2>Z-Test<a class="headerlink" href="#z-test" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The one-sample z-test is used to test whether the mean of a population is greater than, less than, or not equal to a specific value.</p></li>
<li><p>For large (50 or more observations) normally distributed samples, normal distribution tests <strong>are equivalent to the T-Test</strong>.</p></li>
</ul>
<p><strong>Requirements for the Z-test:</strong></p>
<ul class="simple">
<li><p>The mean and standard deviation of the population distribution are known</p></li>
<li><p>The mean of the sample distribution is known</p></li>
<li><p>The variance of the sample is assumed to be the same as the population</p></li>
<li><p>The population is assumed to be normally distributed</p></li>
</ul>
</section>
<section id="t-test">
<h2>T-Test<a class="headerlink" href="#t-test" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Statistical test that is used to compare the means of two groups.</p></li>
<li><p>When the original (population) distribution is not normal, the one sample t-test is still valid with a large enough sample size.</p></li>
<li><p>We perform a <strong>One-Sample t-test</strong> when we want to compare a sample mean with the population mean.</p></li>
<li><p>The difference from the Z Test is that we do not have the information on Population Variance here.</p></li>
<li><p>We use the sample standard deviation instead of population standard deviation in this case.</p></li>
</ul>
<p><strong>One-Sample T-Test:</strong></p>
<ul class="simple">
<li><p>Robust to the normality assumption when the sample size is large enough. Assumptions must be met:</p></li>
<li><p>Samples are drawn from a Gaussian Distribution</p></li>
<li><p>If a two-sample test, both populations are assumed to have the same standard deviation
Observations in the sample are independent of one another</p></li>
</ul>
<p><strong>Rules of Thumb in Evaluating Assumptions:</strong></p>
<ul class="simple">
<li><p>If sample sizes are the same and sufficiently large, the t-tools are valid since they are robust to the violation of normality.</p></li>
<li><p>If the two populations have the same standard deviation then the t-tests are valid given sufficient sample sizes.</p></li>
<li><p>If the standard deviations are different and the sample sizes are different, then the t-tools are not valid and another procedure should be used.</p></li>
</ul>
<p><strong>Transformations</strong></p>
<ul class="simple">
<li><p>If assumptions are not met, look at transforming the data such as taking the logarithmic transformation.</p></li>
</ul>
</section>
<section id="statistical-power">
<h2>Statistical Power<a class="headerlink" href="#statistical-power" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>A power test will tell us how many samples we will need to collect to have a good amount of statistical power.</p></li>
<li><p>It tells you how many trials you need to do to avoid incorrectly rejecting the null hypothesis</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Power</span> <span class="o">=</span> <span class="n">Type</span> <span class="n">II</span> <span class="n">Error</span> <span class="o">=</span> <span class="n">fail</span> <span class="n">to</span> <span class="n">reject</span> <span class="n">a</span> <span class="n">false</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="o">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Power is the probability of not making a Type II error.</p></li>
<li><p>Power = the probability that we correctly reject the null hypothesis (e.g. small p-value). or “the probability of rejecting a null hypothesis when it is false”</p></li>
<li><p>Low Power = when there is a lot of over lap between the two distributions and we have a small sample size, we have low power.</p></li>
<li><p>When we have a lot of power, there is a higher probability that we will correctly reject the null hypothesis.</p></li>
</ul>
</section>
<section id="anova">
<h2>ANOVA<a class="headerlink" href="#anova" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Statistical technique that is used to check if the means of two or more groups are significantly different from each other.</p></li>
</ul>
<p><strong>Assumptions:</strong></p>
<ul class="simple">
<li><p>Normality:</p>
<ul>
<li><p>Similar to Hypothesis Testing, ANOVA is robust to this assumption.</p></li>
<li><p>Extremely long-tailed distributions (outliers) or skewed distributions coupled with different sample sizes (especially when the sample sizes are small) present the only serious distributional problems.</p></li>
</ul>
</li>
<li><p>Equal Standard Deviations:</p>
<ul>
<li><p>This assumption is crucial, paramount, VERY important.</p></li>
<li><p>The assumptions of independence within and across groups are critical.</p></li>
<li><p>If lacking, different analysis should be attempted.</p></li>
</ul>
</li>
</ul>
</section>
<section id="f-test">
<h2>F-Test<a class="headerlink" href="#f-test" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Hypothesis test to check if Evidence of Inequality of Variance</p></li>
</ul>
</section>
<section id="central-limit-theorem">
<h2>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>The distribution of sample x’s will, as the sample size increases, approach a normal distribution.</p></li>
<li><p>The mean (x) of the sample means is the population mean µ.</p></li>
<li><p>The standard deviation of the distribution of sample means is sigma/sqrt(n)</p></li>
</ul>
</section>
<section id="confidence-interval">
<h2>Confidence Interval<a class="headerlink" href="#confidence-interval" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>A 95% confidence interval means that if we were to take 100 different samples and compute a 95% confidence interval for each sample, then approximately 95 of the 100 confidence intervals will contain the true mean value.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CI</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+=</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multicollinearity">
<h2>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Occurs whenever an independent variable is highly correlated with one or more
of the other independent variables in a multiple regression equation.</p></li>
<li><p><strong>Problem:</strong> An independent variable that is very highly correlated with one or more other independent variables will have a relatively large standard error.</p></li>
<li><p>This implies that the partial regression coefficient is unstable and will vary greatly from one sample to the next.</p></li>
<li><p>Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). A VIF &gt;= 10 is problematic.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="model-evaluation">
<h2>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline"></a></h2>
</section>
<hr class="docutils" />
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this headline"></a></h2>
<section id="r-squared">
<h3>R-Squared<a class="headerlink" href="#r-squared" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable(s) in a regression model.</p></li>
<li><p><strong>Problem 1:</strong> Every time you add a predictor to a model, the R-squared
increases, even if due to chance alone. It never decreases. Consequently, a
model with more terms may appear to have a better fit simply because it has
more terms.</p></li>
<li><p><strong>Problem 2:</strong> If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data.
Leads to overfitting and  produces misleadingly high R-squared values and a lessened ability to make predictions.</p></li>
</ul>
</section>
<section id="adjusted-r-squared">
<h3>Adjusted R-Squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Modified version of R-squared that has been adjusted for the number of predictors in the model.</p></li>
<li><p>Increases only if the new term improves the model more than would be expected by chance.</p></li>
<li><p>Decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.</p></li>
<li><p>It is always lower than the R-squared.</p></li>
</ul>
</section>
<section id="root-mean-squared-error">
<h3>Root Mean Squared Error<a class="headerlink" href="#root-mean-squared-error" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values.</p></li>
<li><p>Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit.</p></li>
<li><p>As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable.</p></li>
<li><p><strong>Lower values of RMSE indicate better fit</strong></p></li>
<li><p>RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.</p></li>
</ul>
</section>
<section id="mean-absolute-error">
<h3>Mean Absolute Error<a class="headerlink" href="#mean-absolute-error" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>MAE is not identical to root-mean square error (RMSE), although some
researchers report and interpret it that way.</p></li>
<li><p>MAE is conceptually simpler and also easier to interpret than RMSE: it is simply the average absolute vertical or horizontal distance between each point in a scatter plot and the Y=X line.</p></li>
<li><p>In other words, MAE is the average absolute difference between X and Y.</p></li>
</ul>
</section>
</section>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline"></a></h2>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Accuracy = (TP+TN)/(TP+FP+FN+TN)</p></li>
<li><p>Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.</p></li>
<li><p><strong>Warning:</strong> Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate.</p></li>
</ul>
</section>
<section id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Positive Predictive Rate</p></li>
<li><p>Precision = (TP)/(TP+FP)</p></li>
<li><p>What proportion of predicted Positives is truly Positive?</p></li>
<li><p>Precision is a valid choice of evaluation metric when we want to be very sure of our prediction</p></li>
</ul>
</section>
<section id="recall">
<h3>Recall<a class="headerlink" href="#recall" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>True Positives or Sensitivity</p></li>
<li><p>These are cases in which mode predicted True and label is True</p></li>
<li><p><strong>Example:</strong> When the label is True, how often does the model predict True?</p></li>
</ul>
</section>
<section id="f1-score">
<h3>F1 Score<a class="headerlink" href="#f1-score" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Weighted average of the recall and precision</p></li>
<li><p>F1 score sort of maintains a balance between the precision and recall for your classifier.</p>
<ul>
<li><p>If your precision is low, the F1 is low and if the recall is low again
your F1 score is low.</p></li>
</ul>
</li>
<li><p><strong>Example:</strong> If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.</p></li>
</ul>
</section>
<section id="specificity">
<h3>Specificity<a class="headerlink" href="#specificity" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>True negative = (1 - False Negative Rate)</p></li>
<li><p>Model predicted no and label is False.</p></li>
<li><p><strong>Example:</strong> When the label is False, how often does the model predict False</p></li>
</ul>
</section>
<section id="false-positives">
<h3>False positives<a class="headerlink" href="#false-positives" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>TYPE 1 Error</p></li>
<li><p><strong>Example:</strong> Model predicted yes, but the patient didn’t actually have the disease.</p></li>
</ul>
</section>
<section id="false-negatives">
<h3>False negatives<a class="headerlink" href="#false-negatives" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>TYPE II Error</p></li>
<li><p><strong>Example:</strong> Model predicted no, but the patient actually did have the disease.</p></li>
</ul>
</section>
<section id="auc-score">
<h3>AUC Score<a class="headerlink" href="#auc-score" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>AUC measures the entire two-dimensional area underneath the entire ROC curve ( TPR and FPR)</p></li>
<li><p>It tells how much a model is capable of distinguishing between classes.</p></li>
<li><p>Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.</p></li>
<li><p><strong>Example:</strong> Higher the AUC, then the better the model is at distinguishing between patients with disease and no disease.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>