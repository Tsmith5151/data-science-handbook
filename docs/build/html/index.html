<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Data Science Handbook &mdash; Data Science Handbook 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> Data Science Handbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Data Science Handbook</a><ul>
<li><a class="reference internal" href="#statistics">Statistics</a></li>
<li><a class="reference internal" href="#p-value">P-Value</a></li>
<li><a class="reference internal" href="#z-test">Z-Test</a></li>
<li><a class="reference internal" href="#t-test">T-Test</a></li>
<li><a class="reference internal" href="#statistical-power">Statistical Power</a></li>
<li><a class="reference internal" href="#anova">ANOVA</a></li>
<li><a class="reference internal" href="#f-test">F-Test</a></li>
<li><a class="reference internal" href="#central-limit-theorem">Central Limit Theorem</a></li>
<li><a class="reference internal" href="#confidence-interval">Confidence Interval</a></li>
<li><a class="reference internal" href="#multicollinearity">Multicollinearity</a></li>
<li><a class="reference internal" href="#data-preprocessing">Data Preprocessing</a></li>
<li><a class="reference internal" href="#missing-values">Missing Values</a></li>
<li><a class="reference internal" href="#outliers">Outliers</a></li>
<li><a class="reference internal" href="#categorical-encoding">Categorical Encoding</a></li>
<li><a class="reference internal" href="#data-normalization">Data Normalization</a></li>
<li><a class="reference internal" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li><a class="reference internal" href="#machine-learning">Machine Learning</a></li>
<li><a class="reference internal" href="#bias-variance-trade-off">Bias Variance Trade-off</a></li>
<li><a class="reference internal" href="#overfitting">Overfitting</a></li>
<li><a class="reference internal" href="#regularization">Regularization</a></li>
<li><a class="reference internal" href="#cross-validation">Cross Validation</a></li>
<li><a class="reference internal" href="#distance-measurements">Distance Measurements</a></li>
<li><a class="reference internal" href="#cross-entropy">Cross Entropy</a></li>
<li><a class="reference internal" href="#loss-functions">Loss Functions</a></li>
<li><a class="reference internal" href="#discriminative-vs-generative">Discriminative vs Generative</a></li>
<li><a class="reference internal" href="#feature-selection">Feature Selection</a></li>
<li><a class="reference internal" href="#clustering">Clustering</a></li>
<li><a class="reference internal" href="#overview-sklearn">Overview (Sklearn)</a></li>
<li><a class="reference internal" href="#selecting-cluster-algorithm">Selecting Cluster Algorithm</a></li>
<li><a class="reference internal" href="#k-means-clustering">K-Means Clustering</a></li>
<li><a class="reference internal" href="#evaluation-of-clusters">Evaluation of Clusters</a></li>
<li><a class="reference internal" href="#hierarchical-clustering">Hierarchical Clustering</a></li>
<li><a class="reference internal" href="#dbscan">DBSCAN</a></li>
<li><a class="reference internal" href="#affinity-propagation">Affinity Propagation</a></li>
<li><a class="reference internal" href="#machine-learning-algorithms">Machine Learning Algorithms</a></li>
<li><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li><a class="reference internal" href="#k-nearest-neighbor">K-Nearest Neighbor</a></li>
<li><a class="reference internal" href="#support-vector-machines">Support Vector Machines</a></li>
<li><a class="reference internal" href="#naive-bayes">Naive Bayes</a></li>
<li><a class="reference internal" href="#decision-tree">Decision Tree</a></li>
<li><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li><a class="reference internal" href="#ada-boost">ADA Boost</a></li>
<li><a class="reference internal" href="#xgboost">XGBoost</a></li>
<li><a class="reference internal" href="#lightgbm">LightGBM</a></li>
<li><a class="reference internal" href="#survival-analysis">Survival Analysis</a></li>
<li><a class="reference internal" href="#model-evaluation">Model Evaluation</a></li>
<li><a class="reference internal" href="#regression">Regression</a><ul>
<li><a class="reference internal" href="#r-squared">R-Squared</a></li>
<li><a class="reference internal" href="#adjusted-r-squared">Adjusted R-Squared</a></li>
<li><a class="reference internal" href="#root-mean-squared-error">Root Mean Squared Error</a></li>
<li><a class="reference internal" href="#mean-absolute-error">Mean Absolute Error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">Classification</a><ul>
<li><a class="reference internal" href="#accuracy">Accuracy</a></li>
<li><a class="reference internal" href="#precision">Precision</a></li>
<li><a class="reference internal" href="#recall">Recall</a></li>
<li><a class="reference internal" href="#f1-score">F1 Score</a></li>
<li><a class="reference internal" href="#specificity">Specificity</a></li>
<li><a class="reference internal" href="#false-positives">False positives</a></li>
<li><a class="reference internal" href="#false-negatives">False negatives</a></li>
<li><a class="reference internal" href="#auc-score">AUC Score</a></li>
</ul>
</li>
<li><a class="reference internal" href="#production-monitoring">Production Monitoring</a></li>
<li><a class="reference internal" href="#a-b-testing">A/B Testing</a></li>
<li><a class="reference internal" href="#data-drift">Data Drift</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Data Science Handbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Data Science Handbook</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="data-science-handbook">
<h1>Data Science Handbook<a class="headerlink" href="#data-science-handbook" title="Permalink to this headline">ÔÉÅ</a></h1>
<hr class="docutils" />
<section id="statistics">
<h2>Statistics<a class="headerlink" href="#statistics" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="p-value">
<h2>P-Value<a class="headerlink" href="#p-value" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Statistically significant is detected when the observed p-value from the test statistic is less than the level of significance (i.e. alpha).</p></li>
<li><p>Meaning, there is a very low probability of observing by random chance something as extreme or more extreme than what was observed under the assumption that the null hypothesis is true.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="o">-</span><span class="n">Value</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="o">=</span> <span class="n">Reject</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="p">;</span>
<span class="n">P</span><span class="o">-</span><span class="n">Value</span> <span class="o">&gt;</span> <span class="mf">0.05</span> <span class="o">=</span> <span class="n">Fail</span> <span class="n">to</span> <span class="n">reject</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="p">;</span>
</pre></div>
</div>
<a class="reference external image-reference" href="_static/pvalue.png"><img alt="image" src="_images/pvalue.png" /></a>
<p><strong>Note:</strong> If we reject the null hypothesis, this indicates strong statistical significance.</p>
</section>
<section id="z-test">
<h2>Z-Test<a class="headerlink" href="#z-test" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>The one-sample z-test is used to test whether the mean of a population is greater than, less than, or not equal to a specific value.</p></li>
<li><p>For large (50 or more observations) normally distributed samples, normal distribution tests <strong>are equivalent to the T-Test</strong>.</p></li>
</ul>
<p><strong>Requirements for the Z-test:</strong></p>
<ul class="simple">
<li><p>The mean and standard deviation of the population distribution are known</p></li>
<li><p>The mean of the sample distribution is known</p></li>
<li><p>The variance of the sample is assumed to be the same as the population</p></li>
<li><p>The population is assumed to be normally distributed</p></li>
</ul>
<a class="reference external image-reference" href="_static/ztest.png"><img alt="image" src="_images/ztest.png" /></a>
</section>
<section id="t-test">
<h2>T-Test<a class="headerlink" href="#t-test" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Statistical test that is used to compare the means of two groups.</p></li>
<li><p>When the original (population) distribution is not normal, the one sample t-test is still valid with a large enough sample size.</p></li>
<li><p>We perform a <strong>One-Sample t-test</strong> when we want to compare a sample mean with the population mean.</p></li>
<li><p>The difference from the Z Test is that we do not have the information on Population Variance here.</p></li>
<li><p>We use the sample standard deviation instead of population standard deviation in this case.</p></li>
</ul>
<p><strong>One-Sample T-Test:</strong></p>
<ul class="simple">
<li><p>Robust to the normality assumption when the sample size is large enough. Assumptions must be met:</p></li>
<li><p>Samples are drawn from a Gaussian Distribution</p></li>
<li><p>If a two-sample test, both populations are assumed to have the same standard deviation
Observations in the sample are independent of one another</p></li>
</ul>
<p><strong>Rules of Thumb in Evaluating Assumptions:</strong></p>
<ul class="simple">
<li><p>If sample sizes are the same and sufficiently large, the t-tools are valid since they are robust to the violation of normality.</p></li>
<li><p>If the two populations have the same standard deviation then the t-tests are valid given sufficient sample sizes.</p></li>
<li><p>If the standard deviations are different and the sample sizes are different, then the t-tools are not valid and another procedure should be used.</p></li>
</ul>
<p><strong>Transformations</strong></p>
<ul class="simple">
<li><p>If assumptions are not met, look at transforming the data such as taking the logarithmic transformation.</p></li>
</ul>
</section>
<section id="statistical-power">
<h2>Statistical Power<a class="headerlink" href="#statistical-power" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>A power test will tell us how many samples we will need to collect to have a good amount of statistical power.</p></li>
<li><p>It tells you how many trials you need to do to avoid incorrectly rejecting the null hypothesis</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Power</span> <span class="o">=</span> <span class="n">Type</span> <span class="n">II</span> <span class="n">Error</span> <span class="o">=</span> <span class="n">fail</span> <span class="n">to</span> <span class="n">reject</span> <span class="n">a</span> <span class="n">false</span> <span class="n">null</span> <span class="n">hypothesis</span><span class="o">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Power is the probability of not making a Type II error.</p></li>
<li><p>Power = the probability that we correctly reject the null hypothesis (e.g. small p-value). or ‚Äúthe probability of rejecting a null hypothesis when it is false‚Äù</p></li>
<li><p>Low Power = when there is a lot of over lap between the two distributions and we have a small sample size, we have low power.</p></li>
<li><p>When we have a lot of power, there is a higher probability that we will correctly reject the null hypothesis.</p></li>
</ul>
<a class="reference external image-reference" href="_static/power.png"><img alt="image" src="_images/power.png" /></a>
</section>
<section id="anova">
<h2>ANOVA<a class="headerlink" href="#anova" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Statistical technique that is used to check if the means of two or more groups are significantly different from each other.</p></li>
</ul>
<p><strong>Assumptions:</strong></p>
<ul class="simple">
<li><p>Normality:</p>
<ul>
<li><p>Similar to Hypothesis Testing, ANOVA is robust to this assumption.</p></li>
<li><p>Extremely long-tailed distributions (outliers) or skewed distributions coupled with different sample sizes (especially when the sample sizes are small) present the only serious distributional problems.</p></li>
</ul>
</li>
<li><p>Equal Standard Deviations:</p>
<ul>
<li><p>This assumption is crucial, paramount, VERY important.</p></li>
<li><p>The assumptions of independence within and across groups are critical.</p></li>
<li><p>If lacking, different analysis should be attempted.</p></li>
</ul>
</li>
</ul>
</section>
<section id="f-test">
<h2>F-Test<a class="headerlink" href="#f-test" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Hypothesis test to check if Evidence of Inequality of Variance</p></li>
</ul>
</section>
<section id="central-limit-theorem">
<h2>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>The distribution of sample x‚Äôs will, as the sample size increases, approach a normal distribution.</p></li>
<li><p>The mean (x) of the sample means is the population mean ¬µ.</p></li>
<li><p>The standard deviation of the distribution of sample means is sigma/sqrt(n)</p></li>
</ul>
</section>
<section id="confidence-interval">
<h2>Confidence Interval<a class="headerlink" href="#confidence-interval" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>A 95% confidence interval means that if we were to take 100 different samples and compute a 95% confidence interval for each sample, then approximately 95 of the 100 confidence intervals will contain the true mean value.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CI</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+=</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multicollinearity">
<h2>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Occurs whenever an independent variable is highly correlated with one or more
of the other independent variables in a multiple regression equation.</p></li>
<li><p><strong>Problem:</strong> An independent variable that is very highly correlated with one or more other independent variables will have a relatively large standard error.</p></li>
<li><p>This implies that the partial regression coefficient is unstable and will vary greatly from one sample to the next.</p></li>
<li><p>Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). A VIF &gt;= 10 is problematic.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="data-preprocessing">
<h2>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="missing-values">
<h2>Missing Values<a class="headerlink" href="#missing-values" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Reasons:</p>
<ul>
<li><p>Missing completely at random (MCAR)</p></li>
<li><p>Missing at random (MAR)</p></li>
<li><p>Not missing at random (NMAR)</p></li>
</ul>
</li>
<li><p>How to handle the missing values:</p>
<ul>
<li><p>Do Nothing:</p>
<ul>
<li><p>Models like XGBoost can deal with missing values by deciding for each sample which is the best way to impute them and learns the best values</p></li>
</ul>
</li>
<li><p>Imputation:</p>
<ul>
<li><p>Using (Mean/Median) Value</p></li>
<li><p>Using (Most Frequent) Value</p></li>
<li><p>Using k-NN</p></li>
<li><p>Interpolation (Linear/Nearest Neighbors)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="outliers">
<h2>Outliers<a class="headerlink" href="#outliers" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>Identify Outliers</strong></p>
<ul class="simple">
<li><p><strong>Cook‚Äôs Distance:</strong></p>
<ul>
<li><p>Measures the effect of deleting a given observation. It represents the sum of all the changes in the regression model when observation ‚Äúi‚Äù is removed from it.</p></li>
</ul>
</li>
<li><p><strong>Interquartile Range Method (IQR):</strong></p>
<ul>
<li><p>Is a good statistic for summarizing a non-Gaussian distribution sample of data.</p></li>
<li><p>IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot.</p></li>
<li><p>The IQR defines the middle 50% of the data, or the body of the data</p></li>
<li><p>Can be used to identify outliers by defining limits on the sample values that are below the 25th percentile or above the 75th percentile.</p></li>
<li><p>Linear Models: Projection methods that model the data into lower dimensions using linear correlations.</p></li>
<li><p>For example, PCA and data with large residual errors may be outliers.</p></li>
<li><p>Proximity-based Models: Data instances that are isolated from the mass of the data as determined by cluster, density or KNN analysis.</p></li>
</ul>
</li>
</ul>
<p><strong>Handling Outliers</strong></p>
<ul class="simple">
<li><p><strong>Log-Scale Transformation:</strong> This method is often used to reduce the variability of data including outlying observation.</p></li>
<li><p><strong>Model Selection:</strong> Tree based models are less impacted by outliers compared to linear models.</p>
<ul>
<li><p>XGBoost and boosting in general are very sensitive to outliers.</p></li>
<li><p>This is because boosting builds each tree on previous trees‚Äô residuals/errors.</p></li>
</ul>
</li>
<li><p>Outliers will have much larger residuals than non-outliers, so boosting will focus a disproportionate amount of its attention on those points</p></li>
</ul>
</section>
<section id="categorical-encoding">
<h2>Categorical Encoding<a class="headerlink" href="#categorical-encoding" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>One Hot Encoding:</strong></p>
<ul class="simple">
<li><p>Maps each category to a vector that contains 1 and 0 denoting the presence or absence of the feature.</p></li>
<li><p>The number of vectors depends on the number of categories for features.</p></li>
<li><p>This method produces a lot of columns that slows down the learning significantly if the number of the category is very high for the feature.</p></li>
</ul>
<p><strong>Label Encoder:</strong></p>
<ul class="simple">
<li><p>Each category is assigned a value from 1 through N (here N is the number of categories for the feature.</p></li>
<li><p>One major issue with this approach is there is no relation or order between these classes, but the algorithm might consider them as some order, or there is some relationship.</p></li>
</ul>
<p><strong>Ordinal encoding:</strong></p>
<ul class="simple">
<li><p>To ensure the encoding of variables retains the ordinal nature of the variable.</p></li>
<li><p>This is reasonable only for ordinal variables.</p></li>
<li><p>The transformation looks almost similar to Label Encoding but slightly different as Label coding would not consider whether a variable is ordinal or not and it will assign a sequence of integers.</p></li>
</ul>
<p><strong>Binary Encoding:</strong></p>
<ul class="simple">
<li><p>Converts a category into binary digits.</p></li>
<li><p>Each binary digit creates one feature column.</p></li>
<li><p>If there are n unique categories, then binary encoding results in the only log(base 2)‚Åø features.</p></li>
<li><p>Compared to One Hot Encoding, this will require fewer feature columns.</p>
<ul>
<li><p><strong>Explain:</strong> for 100 categories One Hot Encoding will have 100 features
while forBinary encoding, we will need just seven features.</p></li>
</ul>
</li>
</ul>
</section>
<section id="data-normalization">
<h2>Data Normalization<a class="headerlink" href="#data-normalization" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Standardize: scaling features by removing the mean and scaling to unit
variance</p></li>
<li><p>MinMax: Transform features by scaling each feature to a given range [-1,1].</p></li>
</ul>
</section>
<section id="dimensionality-reduction">
<h2>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>Principal Component Analysis</strong></p>
<ul class="simple">
<li><p>Dimension reduction technique that finds the variance maximizing directions onto which to project the data.</p></li>
<li><p>Algorithm to reduce the dimensionality of the data by compressing it onto a new feature subspace, where a subset of the principal components (i.e. eigenvectors) accounts for the highest variance and explains the underlying structure of the overall dataset.</p></li>
<li><p>The eigenvectors of the correlation or covariance matrix represent the principal components (directions of maximum variance and determine the direction of the new feature space) and the eigenvalues (scalar) correspond to the magnitude of the eigenvectors.</p></li>
<li><p>The eigenvector with the largest eigenvalue is the direction along which the data set has the maximum variance.</p></li>
<li><p>After applying the linear PCA transformation, we have a lower dimensional subspace where the samples are ‚Äúmost spread‚Äù along the new feature axes.</p></li>
<li><p>PCA reduces high dimensional space down to two or three principal components without losing much information.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="machine-learning">
<h2>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="bias-variance-trade-off">
<h2>Bias Variance Trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><strong>Bias:</strong> is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.</p></li>
<li><p><strong>Variance:</strong> is the variability of a model prediction for a given data point.</p></li>
<li><p>The sweet spot for any model is the level of complexity at which the increase in bias is equivalent to the reduction in variance.</p></li>
<li><p>Increasing model complexity tends to increase variance and decrease bias.</p></li>
<li><p>However our model complexity exceeds this sweet spot we are in effect
over-fitting; while if our complexity falls short of the sweet spot =
under-fitting</p></li>
</ul>
<a class="reference external image-reference" href="_static/bias_variance.png"><img alt="image" src="_images/bias_variance.png" /></a>
<p><strong>Addressing Variance:</strong></p>
<ul class="simple">
<li><p>Bagging and other resampling techniques can be used to reduce the variance in
model predictions.</p></li>
<li><p>In bagging (Bootstrap Aggregating), numerous replicates of the original data set are created using random selection with replacement.</p></li>
</ul>
</section>
<section id="overfitting">
<h2>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Occurs when the model over fits on the training data and does not generalize
to the unseen sample population.</p></li>
<li><p>Ways to address overfitting:</p>
<ul>
<li><p>Get more data</p></li>
<li><p>Add regularization</p></li>
<li><p>Cross-Validation</p></li>
<li><p>Less complex model</p></li>
<li><p>Data augmentation (images)</p></li>
<li><p>Smaller input dimensionality (remove features)</p></li>
</ul>
</li>
</ul>
</section>
<section id="regularization">
<h2>Regularization<a class="headerlink" href="#regularization" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Technique to help reduce overfitting by adding an additional parameter to the loss function, usually the L1 or L2 norm.</p></li>
<li><p>In order to help prevent overfitting, we can add in a term into our optimization that keeps the weights small</p></li>
</ul>
<p><strong>L1 Regularization (Lasso): ‚ÄúAbsolute Value Magnitude‚Äù</strong></p>
<ul class="simple">
<li><p>Lasso Regularizer forces a lot of feature weights to be zero</p></li>
</ul>
<p><strong>L2 Regularization (Ridge): ‚ÄúSquared Magnitude‚Äù</strong></p>
<a class="reference external image-reference" href="_static/regularization.png"><img alt="image" src="_images/regularization.png" /></a>
</section>
<section id="cross-validation">
<h2>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>To avoid sampling issues, which can cause the training-set to be too optimistic.</p></li>
<li><p>Cross-validation is used to protect against overfitting in a predictive model, particularly the case where the amount of data is limited.</p></li>
</ul>
<p><strong>K-Fold:</strong></p>
<ul class="simple">
<li><p>Splits the training data into ‚Äòk‚Äô folds to validate the model on one file while training on the k-1 other folds ‚Äòk‚Äô times. The error is then averages over the fold</p></li>
</ul>
</section>
<section id="distance-measurements">
<h2>Distance Measurements<a class="headerlink" href="#distance-measurements" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>Euclidean Distance</strong></p>
<ul class="simple">
<li><p>sqrt((x2-x1)2 + (y2-y2)2)‚Äì&gt; Pythagorean Theorem</p></li>
</ul>
<p><strong>Manhattan Distance</strong></p>
<ul class="simple">
<li><p>Calculates the distance between two data points in a grid like path - absolute sum of difference</p></li>
</ul>
<p><strong>Cosine Distance</strong></p>
<ul class="simple">
<li><p>Measure the degree of angle between two documents or vectors.</p></li>
<li><p>Cosine value 1 is for vectors pointing in the same direction i.e. there are
similarities between the documents/data points.</p></li>
<li><p>At zero for orthogonal vectors i.e. Unrelated(some similarity found).</p></li>
<li><p>Value -1 for vectors pointing in opposite directions(No similarity).</p></li>
</ul>
<p><strong>Mahalanobis Distance</strong></p>
<ul class="simple">
<li><p>A measure of the distance between a point P and a distribution D.</p></li>
<li><p>Why use it?</p>
<ul>
<li><p>If the feature vectors are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.</p></li>
<li><p>Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary</p></li>
</ul>
</li>
<li><p>Steps:</p>
<ul>
<li><p>It transforms the columns into uncorrelated variables</p></li>
<li><p>Scale the columns to make their variance equal to 1</p></li>
<li><p>Finally, it calculates the Euclidean distance.</p></li>
</ul>
</li>
</ul>
</section>
<section id="cross-entropy">
<h2>Cross Entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Measures the difference between two probability distributions for a given random variable or set of events - classification problems</p></li>
<li><p>Entropy: is the number of bits required to transmit a randomly selected event
from a probability distribution.</p></li>
<li><p>A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.</p></li>
</ul>
<a class="reference external image-reference" href="_static/cross_entropy1.png"><img alt="image" src="_images/cross_entropy1.png" /></a>
<a class="reference external image-reference" href="_static/cross_entropy2.png"><img alt="image" src="_images/cross_entropy2.png" /></a>
</section>
<section id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Function that takes as inputs the predicted value ‚Äòz‚Äô corresponding to the
real data value ‚Äòy‚Äô and outputs how different they are.</p></li>
</ul>
<a class="reference external image-reference" href="_static/loss_functions.png"><img alt="image" src="_images/loss_functions.png" /></a>
</section>
<section id="discriminative-vs-generative">
<h2>Discriminative vs Generative<a class="headerlink" href="#discriminative-vs-generative" title="Permalink to this headline">ÔÉÅ</a></h2>
<a class="reference external image-reference" href="_static/discrimitive_generative.png"><img alt="image" src="_images/discrimitive_generative.png" /></a>
</section>
<section id="feature-selection">
<h2>Feature Selection<a class="headerlink" href="#feature-selection" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>Filter Based</strong></p>
<ul class="simple">
<li><p>We specify some metric and based on that filter features.</p>
<ul>
<li><p>chi-square test</p></li>
<li><p>fisher score</p></li>
<li><p>correlation coefficient</p></li>
<li><p>variance threshold</p></li>
</ul>
</li>
</ul>
<p><strong>Wrapper-based</strong></p>
<ul class="simple">
<li><p>Wrapper methods consider the selection of a set of features as a search problem.</p>
<ul>
<li><p>Sequential Feature Selection</p></li>
<li><p>Forward/Stepwise/Backward Selection</p></li>
</ul>
</li>
</ul>
<p><strong>Embedded</strong></p>
<ul class="simple">
<li><p>Embedded methods use algorithms that have built-in feature selection methods.</p>
<ul>
<li><p>Lasso</p></li>
<li><p>Tree based models</p></li>
</ul>
</li>
</ul>
<p><strong>Forward Selection</strong></p>
<ul class="simple">
<li><p>The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.</p></li>
</ul>
<p><strong>Backward Elimination</strong></p>
<ul class="simple">
<li><p>The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.</p></li>
</ul>
<p><strong>Sequential Feature Selection</strong></p>
<ul class="simple">
<li><p>Greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based on a cross-validation score.</p></li>
<li><p>That is, we start with 0 features and choose the best single feature with the highest score.</p></li>
<li><p>The procedure is repeated until we reach the desired number of selected features.</p></li>
</ul>
<p><strong>Embedded Feature Selection</strong></p>
<ul class="simple">
<li><p>Augment with noisy data</p></li>
<li><p>Can apply this approach to Tree based models: XGBoost, DecisionTree, RandomForrest</p></li>
<li><p>The idea is that we can inject noisy data features into our input training dataset when training model</p></li>
<li><p>Here we can perform cross-validation with n-folds where at each n-fold inject noisy features in each iteration and determine the threshold in which the first noisy feature is selected when computing feature importance.</p></li>
<li><p>We select the raw features that are less than this threshold and append to a
list.</p></li>
<li><p>We do this iteratively for each fold and then take a set of the final appended list.</p></li>
<li><p>This approach can help minimize model overfitting.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="overview-sklearn">
<h2>Overview (Sklearn)<a class="headerlink" href="#overview-sklearn" title="Permalink to this headline">ÔÉÅ</a></h2>
<a class="reference external image-reference" href="_static/cluster1.png"><img alt="image" src="_images/cluster1.png" /></a>
</section>
<section id="selecting-cluster-algorithm">
<h2>Selecting Cluster Algorithm<a class="headerlink" href="#selecting-cluster-algorithm" title="Permalink to this headline">ÔÉÅ</a></h2>
<a class="reference external image-reference" href="_static/cluster2.png"><img alt="image" src="_images/cluster2.png" /></a>
</section>
<section id="k-means-clustering">
<h2>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>KMeans, is an iterative process and aims to partition ‚ÄòN‚Äô observations into
‚ÄòK‚Äô where observation belongs to the cluster with the closest mean.</p></li>
<li><p>By assigning ‚ÄòN‚Äô observations to ‚ÄòK‚Äô clusters such that within each the average dissimilarity of the observations from the cluster mean (i.e. centroid) is minimized.</p></li>
<li><p>Specify the number of clusters ‚Äòk‚Äô</p></li>
<li><p>Randomly pick k centroids from the data points as initial cluster centers
Assign each sample to the nearest centroid (i.e. Euclidean distance)</p></li>
<li><p>Move the centroids to the center of the samples that were assigned to it
Repeat the third and fourth steps until the cluster assignment converges</p></li>
</ul>
<a class="reference external image-reference" href="_static/kmeans.png"><img alt="image" src="_images/kmeans.png" /></a>
</section>
<section id="evaluation-of-clusters">
<h2>Evaluation of Clusters<a class="headerlink" href="#evaluation-of-clusters" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>Elbow method:</strong></p>
<ul class="simple">
<li><p>plot number of clusters versus interia</p></li>
<li><p><strong>Inertia:</strong> measure of how internally coherent clusters are</p></li>
<li><p><strong>Cohesion:</strong> measures how closely related are objects in a cluster</p></li>
<li><p><strong>Separation:</strong> measures how distinctly separated a cluster is from other clusters</p></li>
</ul>
<p><strong>Calinski-Harabasz:</strong></p>
<ul class="simple">
<li><p>Also known as the Variance Ratio Criterion</p></li>
<li><p>Indicates how well a clustering model defines its clusters, such that the higher the score, the more dense and well separated the clusters are.</p></li>
</ul>
<p><strong>Silhouette Coefficient:</strong></p>
<ul class="simple">
<li><p>Is calculated using the mean intra-cluster distance
(a) and the mean nearest-cluster distance (b) for each sample.</p>
<ul>
<li><p>The Silhouette Coefficient for a sample is (b - a) / max(a, b)</p></li>
<li><p>The best value is 1 and the worst value is -1.</p></li>
<li><p>Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</p></li>
</ul>
</li>
</ul>
<p><strong>Difference between KNN and KMeans</strong></p>
<ul class="simple">
<li><p>K-NN is a Supervised machine learning while K-means is an unsupervised machine learning.</p></li>
<li><p>K-NN is a classification or regression machine learning algorithm while</p></li>
<li><p>K-NN performs much better if all of the data have the same scale but this is not true for K-means.</p></li>
<li><p>K-means is a clustering machine learning algorithm and used for unsupervised ML tasks.</p></li>
</ul>
</section>
<section id="hierarchical-clustering">
<h2>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Agglomerative Clustering is a clustering algorithm that builds nested
clusters by merging or splitting them successively.</p></li>
<li><p>This hierarchy of clusters is represented as a tree (or dendrogram).</p></li>
<li><p>Hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together.
Hierarchical clustering algorithms aims at optimizing different objective functions:</p></li>
<li><p><strong>Ward</strong> minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.</p></li>
<li><p><strong>Maximum or complete linkage:</strong> minimizes the maximum distance between observations of pairs of clusters.</p></li>
<li><p><strong>Average linkage minimizes</strong> the average of the distances between all observations of pairs of clusters.</p></li>
<li><p><strong>Single linkage</strong> minimizes the distance between the closest observations of pairs of clusters.</p></li>
</ul>
</section>
<section id="dbscan">
<h2>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Clustering algorithm views clusters as areas of high density separated by areas of low density.</p></li>
<li><p>Clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped.</p></li>
</ul>
</section>
<section id="affinity-propagation">
<h2>Affinity Propagation<a class="headerlink" href="#affinity-propagation" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Affinity Propagation creates clusters by sending messages between pairs of samples until convergence.</p></li>
<li><p>The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs.</p></li>
<li><p>This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="machine-learning-algorithms">
<h2>Machine Learning Algorithms<a class="headerlink" href="#machine-learning-algorithms" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Multi-variable linear equations might look like this, where ùë§ represents the coefficients, or weights, our model will try to learn.</p></li>
<li><p><strong>Goal:</strong> minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.</p></li>
</ul>
<a class="reference external image-reference" href="_static/linear_regression1.png"><img alt="image" src="_images/linear_regression1.png" /></a>
<ul class="simple">
<li><p><strong>Loss Function:</strong> To minimize MSE (or L2 Loss) we use Gradient Descent to calculate the gradient of our cost function:</p></li>
</ul>
<a class="reference external image-reference" href="_static/linear_regression2.png"><img alt="image" src="_images/linear_regression2.png" /></a>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.</p></li>
</ul>
<a class="reference external image-reference" href="_static/logistic_regression.png"><img alt="image" src="_images/logistic_regression.png" /></a>
<p><strong>Loss Function</strong>:  Cross-Entropy</p>
<ul class="simple">
<li><p>Same process as Linear Regression, except we replace the sigmoid function with the softmax function.</p></li>
<li><p>Why don‚Äôt we use MSE for classification problems?</p>
<ul>
<li><p>Our prediction function is nonlinear (due to sigmoid transform).</p></li>
<li><p>Squaring this prediction as we do in MSE results in a non-convex function with many local minimums.</p></li>
<li><p>If our cost function has many local minimums, gradient descent may not find the optimal global minimum.</p></li>
</ul>
</li>
</ul>
</section>
<section id="k-nearest-neighbor">
<h2>K-Nearest Neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>An iterative clustering algorithm that groups samples which consist of similar characteristics and that are more related to each other than in other groups.</p></li>
<li><p>Each group in the data is distributed around a central point called the ‚Äúcentroid‚Äù which is the average of the cluster.</p></li>
</ul>
<a class="reference external image-reference" href="_static/knn.png"><img alt="image" src="_images/knn.png" /></a>
<p><strong>Steps</strong>:</p>
<ul class="simple">
<li><p>Specify the number of clusters ‚Äòk‚Äô</p></li>
<li><p>Randomly pick k centroids from the data points as initial cluster centers</p></li>
<li><p>Assign each sample to the nearest centroid (i.e. Euclidean distance)</p></li>
<li><p>Move the centroids to the center of the samples that were assigned to it</p></li>
<li><p>Repeat the third and fourth steps until the cluster assignment converges</p></li>
</ul>
</section>
<section id="support-vector-machines">
<h2>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>The goal of support vector machines is to find the line that maximizes the minimum distance to the line.</p></li>
<li><p>The decision boundary is defined as: w^Tx - b</p></li>
</ul>
<a class="reference external image-reference" href="_static/svm1.png"><img alt="image" src="_images/svm1.png" /></a>
<a class="reference external image-reference" href="_static/svm2.png"><img alt="image" src="_images/svm2.png" /></a>
<p><strong>Kernel Trick:</strong> -</p>
<ul class="simple">
<li><p>Non-linear separable -&gt; kernel mapping -&gt; decision boundary in original space</p></li>
<li><p>The ‚Äúkernel trick‚Äù is used to compute the cost function using the kernel because we actually don‚Äôt need to know the explicit mapping œï, which is often very</p></li>
</ul>
<a class="reference external image-reference" href="_static/svm3.png"><img alt="image" src="_images/svm3.png" /></a>
</section>
<section id="naive-bayes">
<h2>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Naive Bayes is a probabilistic algorithm that is based on Bayes Theorem.</p></li>
<li><p>Bayes‚Äô Theorem basically describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature.</p></li>
<li><p>For instance, the conditional probability of B given A.</p></li>
<li><p>Our assumption is the probability that the tag of a sentence is Sports given that the sentence is ‚ÄúA very close game‚Äù</p></li>
</ul>
<a class="reference external image-reference" href="_static/naive_bayes1.png"><img alt="image" src="_images/naive_bayes1.png" /></a>
<p><strong>Example:</strong></p>
<a class="reference external image-reference" href="_static/naive_bayes2.png"><img alt="image" src="_images/naive_bayes2.png" /></a>
<ul class="simple">
<li><p>Classification: Building a classifier that says whether a text is about sports or not</p></li>
<li><p>Since Naive Bayes is a probabilistic classifier, we want to calculate the probability that the sentence ‚ÄúA very close game‚Äù is Sports and the probability that it‚Äôs not.</p></li>
<li><p>Then, we take the largest one. Written mathematically, what we want is P (Sports | a very close game) ‚Äî the probability that the tag of a sentence is Sports given that the sentence is ‚ÄúA very close game‚Äù.</p></li>
</ul>
<a class="reference external image-reference" href="_static/naive_bayes3.png"><img alt="image" src="_images/naive_bayes3.png" /></a>
<ul class="simple">
<li><p>Since for our classifier we‚Äôre just trying to find out which tag has a bigger probability, we can discard the divisor ‚Äîwhich is the same for both tags‚Äî and just compare.</p></li>
</ul>
<a class="reference external image-reference" href="_static/naive_bayes4.png"><img alt="image" src="_images/naive_bayes4.png" /></a>
<ul class="simple">
<li><p>This is better, since we could actually calculate these probabilities!</p></li>
<li><p>Just count how many times the sentence ‚ÄúA very close game‚Äù appears in the Sports tag, divide it by the total, and obtain P.</p></li>
</ul>
<p><strong>Problem</strong>:</p>
<ul class="simple">
<li><p>‚ÄúA very close game‚Äù doesn‚Äôt appear in our training data, so this probability is zero. Unless every sentence that we want to classify appears in our training data, the model won‚Äôt be very useful.</p></li>
<li><p>So here comes the Naive part: we assume that every word in a sentence is independent of the other ones. This means that we‚Äôre no longer looking at entire sentences, but rather at individual words.</p></li>
</ul>
<a class="reference external image-reference" href="_static/naive_bayes5.png"><img alt="image" src="_images/naive_bayes5.png" /></a>
<ul class="simple">
<li><p>This assumption is very strong but super useful. It‚Äôs what makes this model work well with little data or data that may be mislabeled. The next step is just applying this to what we had be
Results:</p></li>
<li><p>In our case, the possible words are [‚Äòa‚Äô, ‚Äògreat‚Äô, ‚Äòvery‚Äô, ‚Äòover‚Äô, ‚Äòit‚Äô, ‚Äòbut‚Äô, ‚Äògame‚Äô, ‚Äòelection‚Äô, ‚Äòclean‚Äô, ‚Äòclose‚Äô, ‚Äòthe‚Äô, ‚Äòwas‚Äô, ‚Äòforgettable‚Äô, ‚Äòmatch‚Äô].</p></li>
</ul>
<a class="reference external image-reference" href="_static/naive_bayes6.png"><img alt="image" src="_images/naive_bayes6.png" /></a>
</section>
<section id="decision-tree">
<h2>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>A decision tree algorithm breaks down our data by making decisions based on asking a series of questions.</p></li>
<li><p>First, the decision tree algorithm is a top-down approach; we start at the tree root and split the data on the feature that results in the largest <strong>information</strong> <strong>gain</strong>.</p></li>
<li><p>It is an iterative process and we can then repeat the splitting criteria at each child node until the leaves are pure.</p></li>
<li><p>To determine how the features are split, we can use the concept of entropy, which measures the uncertainty</p></li>
<li><p>The lower the entropy, the more predictable the class is and for higher entropy values, it becomes more unpredictable.</p></li>
<li><p>Next we compute the difference between the entropies before (i.e. parent node) and after the split (i.e. sub-nodes) yields the information gain.</p></li>
<li><p>Finally, the objective function is to maximize the information gain at each split, thus the attribute with the highest change in entropy is used as the splitting criteria</p></li>
</ul>
<p><strong>Problems with Decision Trees:</strong></p>
<ul class="simple">
<li><p><strong>Overfitting</strong>:</p>
<ul>
<li><p>As the decision tree grows and becomes more complex the issue of overfitting arises. Meaning, the model has virtually memorized the training data but will not be expected to perform well with out-of-sample data.</p></li>
</ul>
</li>
<li><p><strong>Underfitting</strong>:</p>
<ul>
<li><p>If the tree is too simple then this could result in underfitting as the learning value is restricted to one level of the decision tree and does not allow the training set to learn the data adequately; a lower complexity decision tree results in high bias.</p></li>
</ul>
</li>
</ul>
<p><strong>Methods to address problem:</strong></p>
<ul class="simple">
<li><p>We want to prune the tree by setting a limit for the maximum depth of the tree.</p></li>
<li><p>One way is that we can observe the error vs max_depth plots and also implement Gridsearch to identify the optimal depth.</p></li>
</ul>
</section>
<section id="random-forest">
<h2>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Is an ensemble based algorithm that‚Äôs built on the idea of Decision Trees.</p></li>
</ul>
<p><strong>How it Works:</strong></p>
<ul class="simple">
<li><p>Random Forest works by training an ensemble of decision trees where each tree is constructed from a different sample of the original training data.</p>
<ul>
<li><p><strong>Bagging</strong>: where we sample the data with replacement.</p></li>
</ul>
</li>
<li><p>Random Forest uses this sampling technique to reduce the variance in model predictions, by creating many of these trees, in effect a ‚Äúforest‚Äù, and then averaging them.</p></li>
<li><p>The variance of the final model can be greatly reduced compared to a single tree.</p></li>
<li><p>Feature Splitting criteria: choose a random set of features for each split and then compute the entropy and information gain to determine which variable to split on. (see decision tree for more)</p></li>
<li><p>Strength: works well with missing values and outliers.</p></li>
<li><p>Easy to tune for - minimal hyper parameters.</p></li>
<li><p>Weakness: Doesn‚Äôt offer the same level of interpretability as decision trees</p></li>
</ul>
<p><strong>Bagging vs Boosting</strong></p>
<ul class="simple">
<li><p><strong>Bagging</strong></p>
<ul>
<li><p>Sampling with replacement (e.g. some observations may be repeated)</p></li>
<li><p>Is a way to decrease the variance of your prediction by generating additional data for training from your original dataset using combinations with repetitions to produce multisets of the same cardinality/size as your original data.</p></li>
<li><p>By increasing the size of your training set you can‚Äôt improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.</p></li>
</ul>
</li>
</ul>
<a class="reference external image-reference" href="_static/bagging.png"><img alt="image" src="_images/bagging.png" /></a>
<ul class="simple">
<li><p><strong>Boosting</strong></p>
<ul>
<li><p>Boosting involves the creation and addition of decision trees sequentially, each attempting to correct the mistakes of the learners that came before it.</p></li>
<li><p>Instead of training models separately, boosting trains models sequentially, each new model being trained to correct the errors of the previous ones.</p></li>
<li><p>At each iteration (round), the outcomes predicted correctly are given a lower weight, and the ones wrongly predicted a higher weight. It then uses a weighted average to produce a final outcome.</p></li>
<li><p>Unlike bagging, the subset creation is not random and depends upon the performance of the previous models: every new subset contains the elements that were (likely to be) misclassified by previous models.</p></li>
<li><p>Generally, boosting algorithms are configured with weak learners, decision trees with few layers, sometimes as simple as just a root node.</p></li>
</ul>
</li>
</ul>
<a class="reference external image-reference" href="_static/boosting.png"><img alt="image" src="_images/boosting.png" /></a>
<p><strong>Boosting Rounds</strong></p>
<ul class="simple">
<li><p>The general reason is that on most problems, adding more trees beyond a limit does not improve the performance of the model.</p></li>
<li><p>The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. Quickly, the model reaches a point of diminishing returns.</p></li>
</ul>
</section>
<section id="ada-boost">
<h2>ADA Boost<a class="headerlink" href="#ada-boost" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>High weights are put on errors to improve at the next boosting step.</p></li>
</ul>
</section>
<section id="xgboost">
<h2>XGBoost<a class="headerlink" href="#xgboost" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Tree based model that uses the concept of boosting.</p></li>
<li><p>With boosting, we construct a series of trees that attempt to correct the mistakes of the model before it in the sequence.</p></li>
<li><p>The first model is built on training data, the second model improves the first model, the third model improves the second, and so on.</p></li>
<li><p>XGboost also uses <strong>‚ÄúGradient Boosting‚Äù</strong> ‚Äì which is an approach where new models are created that predict the residuals of prior models and then added together to make the final prediction so that the loss function is minimized using gradient descent.</p></li>
<li><p><strong>Loss function:</strong> difference between predicted and actual value</p></li>
<li><p>XGBoost API</p>
<ul>
<li><p><strong>DMatrix</strong>: an internal data structure that is used by XGBoost, which is
optimized for both memory efficiency and training speed.</p></li>
</ul>
</li>
</ul>
<p><strong>Feature Importance:</strong></p>
<ul class="simple">
<li><p>Three ways importance is measured:</p>
<ul>
<li><p><strong>Weight</strong>: The number of times a feature is used to split the data across all trees.</p></li>
<li><p><strong>Cover</strong>: The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits.</p></li>
<li><p><strong>Gain</strong>: The average training loss reduction gained when using a feature for splitting.</p></li>
</ul>
</li>
<li><p>Feature importance orderings are very different for each of the three options provided by XGBoost</p></li>
</ul>
<p><strong>Hyperparameters</strong></p>
<ul class="simple">
<li><p><strong>Learning Rate</strong>: Step size shrinkage used in updates to prevent overfitting.
After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.</p></li>
<li><p><strong>Lambda</strong>: L2 regularization term on weights. Increasing this value will make the model more conservative.</p></li>
<li><p><strong>Alpha</strong>: L1 regularization term on weights. Increasing this value will make the model more conservative.</p></li>
<li><p><strong>Gamma</strong>: Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.</p></li>
<li><p><strong>Max_depth</strong>: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.</p>
<ul>
<li><p>0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth.</p></li>
<li><p>Beware that XGBoost aggressively consumes memory when training a deep tree.</p></li>
</ul>
</li>
<li><p><strong>Subsample</strong>: Subsample ratio of the training instances.</p>
<ul>
<li><p>Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.</p></li>
<li><p>Subsampling will occur once in every boosting iteration.</p></li>
</ul>
</li>
</ul>
<p><strong>Objective Functions</strong></p>
<ul class="simple">
<li><p><strong>multi:softmax:</strong></p>
<ul>
<li><p>Set XGBoost to do multiclass classification using the softmax objective.</p></li>
<li><p>You also need to set num_class(number of classes)</p></li>
</ul>
</li>
<li><p><strong>reg:squarederror:</strong> Regression with squared loss.</p></li>
<li><p><strong>reg:squaredlogerror:</strong>  Regression with squared log loss</p>
<ul>
<li><p>1/2[ùëôùëúùëî(ùëùùëüùëíùëë+1)‚àíùëôùëúùëî(ùëôùëéùëèùëíùëô+1)]^2</p></li>
<li><p>All input labels are required to be greater than -1</p></li>
</ul>
</li>
<li><p><strong>reg:logistic:</strong>  Logistic regression</p></li>
<li><p><strong>binary:logistic:</strong> Logistic regression for binary classification, output probability</p></li>
<li><p><strong>Count:poisson:</strong> Poisson regression for count data, output mean of poisson distribution</p></li>
</ul>
<p><strong>Evaluation Metrics XGB</strong></p>
<ul class="simple">
<li><p><strong>rmse</strong>: root mean square error</p></li>
<li><p><strong>rmsle</strong>: root mean square log error:</p></li>
<li><p>reg:squaredlogerror</p></li>
<li><p>metric reduces errors generated by outliers in the dataset.</p></li>
<li><p><strong>mae</strong>: mean absolute error</p></li>
<li><p><strong>mape</strong>: mean absolute percentage error</p></li>
<li><p><strong>logloss</strong>: negative log-likelihood</p></li>
<li><p><strong>error</strong>: Binary classification error rate.</p>
<ul>
<li><p>It is calculated as #(wrong cases)/#(all cases).</p></li>
<li><p>For the predictions, the evaluation will regard the instances with prediction values larger than 0.5 as positive instances, and the others as negative instances.</p></li>
</ul>
</li>
<li><p><strong>merror</strong>: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).</p></li>
<li><p><strong>mlogloss</strong>: Multiclass logloss.</p></li>
<li><p><strong>auc</strong>: Area under the curve</p></li>
<li><p><strong>map</strong>: Mean Average Precision</p></li>
</ul>
</section>
<section id="lightgbm">
<h2>LightGBM<a class="headerlink" href="#lightgbm" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>An open-source gradient boosting library  developed by Microsoft
LightGBM brings significant improvements to vanilla GBT and can be used to train models on tabular data with incredible speed and accuracy.</p>
<ul>
<li><p>Gradient-based One-Sided Sampling</p></li>
<li><p>Exclusive Feature Bundling</p></li>
</ul>
</li>
</ul>
<p><strong>XGboost Vs LightGBM</strong></p>
<ul class="simple">
<li><p>The main difference between these frameworks is the way they are growing.</p></li>
<li><p><strong>XGBoost</strong>:</p>
<ul>
<li><p>level-wise: an approach where the trees grows horizontal</p></li>
<li><p>level-wise splits based on the contribution to loss of particular branch</p></li>
</ul>
</li>
<li><p><strong>LightGBM</strong></p>
<ul>
<li><p>leaf-wise an approach where the trees grows vertically.</p></li>
<li><p>Leaf-wise splits nodes based on the contribution to global loss whereas.</p></li>
<li><p>leaf-wise is mostly faster than the level-wise (e.g 10x faster than XGboost)</p></li>
</ul>
</li>
</ul>
</section>
<section id="survival-analysis">
<h2>Survival Analysis<a class="headerlink" href="#survival-analysis" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Survival analysis models time to an event of interest.</p></li>
<li><p>It originates from clinical research and is a special kind of regression and differs from the conventional regression task as follows:</p></li>
<li><p>The label is always positive, since you cannot wait a negative amount of time until the event occurs.</p></li>
<li><p>The label may not be fully known, or censored, because ‚Äúit takes time to measure time.‚Äù</p></li>
</ul>
<p><strong>Censoring</strong></p>
<ul class="simple">
<li><p>Some experimenters could not get a complete measurement for that label. We consider this as ‚Äúpartially observed‚Äù</p></li>
<li><p><strong>Example</strong>: if we look at healthcare data and you have a patient who survived the first 30 days and walked out of the clinic on the 31st day, so his death was not directly observed</p></li>
<li><p>Another possibility: The experiment was cut short (since you cannot run it forever) before his death could be observed.</p></li>
<li><p><strong>There are four kinds of censoring:</strong></p>
<ul>
<li><p><strong>Uncensored</strong>: the label is not censored and given as a single number.</p></li>
<li><p><strong>Right-censored:</strong> the label is of form [a,+‚àû), where a is the lower bound.</p></li>
<li><p><strong>Left-censored</strong>: the label is of form [0,b], where b is the upper bound.</p></li>
<li><p><strong>Interval-censored</strong>: the label is of form [a,b], where a and b are the lower and upper bounds, respectively.</p></li>
</ul>
</li>
</ul>
<p><strong>Prepare Training Data:</strong></p>
<ul class="simple">
<li><p>Survival times are subject to right-censoring, therefore, we need to consider an individual‚Äôs status in addition to survival time.</p></li>
<li><p>To structure our training data, we need two fields. The first field indicating whether the actual survival time was observed or if was censored, and the second field denoting the observed survival time, which corresponds to the time of death</p></li>
</ul>
<p><strong>Hazard Function</strong></p>
<ul class="simple">
<li><p>Along with the survival function, we are also interested in the rate at which event is taking place, out of the surviving population at any given time t.</p></li>
<li><p>In medical terms, we can define it as ‚Äúout of the people who survived at time t, what is the rate of dying of those people‚Äù.</p></li>
</ul>
<p><strong>Kaplan‚ÄìMeier</strong></p>
<ul class="simple">
<li><p>Since we don‚Äôt have the true survival curve of the population, thus we will estimate the survival curve from the data.</p></li>
<li><p>KM is a non-parametric statistic used to estimate the survival function from lifetime data. The survival curve is defined as the probability of surviving in a given length of time.</p></li>
<li><p>If we choose not to include the censored data, then it is highly likely that our estimates would be highly biased and under-estimated.</p></li>
<li><p>The inclusion of censored data to calculate the estimates, makes the Survival Analysis very powerful.</p></li>
</ul>
<a class="reference external image-reference" href="_static/survival_analysis1.png"><img alt="image" src="_images/survival_analysis1.png" /></a>
<p><strong>Accelerated Failure Time</strong></p>
<ul class="simple">
<li><p>The first step is to express the labels in the form of a range, so that every data point has two numbers associated with it, namely the lower and upper bounds for the label.
<a class="reference external" href="https://xgboost.readthedocs.io/en/latest/tutorials/aft_survival_analysis.html">https://xgboost.readthedocs.io/en/latest/tutorials/aft_survival_analysis.html</a></p></li>
</ul>
<a class="reference external image-reference" href="_static/survival_analysis2.png"><img alt="image" src="_images/survival_analysis2.png" /></a>
<p><strong>XGBoost Survival Embeddings</strong></p>
<ul class="simple">
<li><p>Apply Kaplan-Meier on nearest neighbors to generate SA curves for each nearest neighbor segmentation.</p></li>
</ul>
<a class="reference external image-reference" href="_static/survival_analysis3.png"><img alt="image" src="_images/survival_analysis3.png" /></a>
</section>
<hr class="docutils" />
<section id="model-evaluation">
<h2>Model Evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="r-squared">
<h3>R-Squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>Statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable(s) in a regression model.</p></li>
<li><p><strong>Problem 1:</strong> Every time you add a predictor to a model, the R-squared
increases, even if due to chance alone. It never decreases. Consequently, a
model with more terms may appear to have a better fit simply because it has
more terms.</p></li>
<li><p><strong>Problem 2:</strong> If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data.
Leads to overfitting and  produces misleadingly high R-squared values and a lessened ability to make predictions.</p></li>
</ul>
</section>
<section id="adjusted-r-squared">
<h3>Adjusted R-Squared<a class="headerlink" href="#adjusted-r-squared" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>Modified version of R-squared that has been adjusted for the number of predictors in the model.</p></li>
<li><p>Increases only if the new term improves the model more than would be expected by chance.</p></li>
<li><p>Decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it‚Äôs usually not.</p></li>
<li><p>It is always lower than the R-squared.</p></li>
</ul>
</section>
<section id="root-mean-squared-error">
<h3>Root Mean Squared Error<a class="headerlink" href="#root-mean-squared-error" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>The square root of the variance of the residuals. It indicates the absolute fit of the model to the data‚Äìhow close the observed data points are to the model‚Äôs predicted values.</p></li>
<li><p>Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit.</p></li>
<li><p>As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable.</p></li>
<li><p><strong>Lower values of RMSE indicate better fit</strong></p></li>
<li><p>RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.</p></li>
</ul>
</section>
<section id="mean-absolute-error">
<h3>Mean Absolute Error<a class="headerlink" href="#mean-absolute-error" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>MAE is not identical to root-mean square error (RMSE), although some
researchers report and interpret it that way.</p></li>
<li><p>MAE is conceptually simpler and also easier to interpret than RMSE: it is simply the average absolute vertical or horizontal distance between each point in a scatter plot and the Y=X line.</p></li>
<li><p>In other words, MAE is the average absolute difference between X and Y.</p></li>
</ul>
</section>
</section>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>Accuracy = (TP+TN)/(TP+FP+FN+TN)</p></li>
<li><p>Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.</p></li>
<li><p><strong>Warning:</strong> Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate.</p></li>
</ul>
</section>
<section id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>Positive Predictive Rate</p></li>
<li><p>Precision = (TP)/(TP+FP)</p></li>
<li><p>What proportion of predicted Positives is truly Positive?</p></li>
<li><p>Precision is a valid choice of evaluation metric when we want to be very sure of our prediction</p></li>
</ul>
</section>
<section id="recall">
<h3>Recall<a class="headerlink" href="#recall" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>True Positives or Sensitivity</p></li>
<li><p>These are cases in which mode predicted True and label is True</p></li>
<li><p><strong>Example:</strong> When the label is True, how often does the model predict True?</p></li>
</ul>
</section>
<section id="f1-score">
<h3>F1 Score<a class="headerlink" href="#f1-score" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>Weighted average of the recall and precision</p></li>
<li><p>F1 score sort of maintains a balance between the precision and recall for your classifier.</p>
<ul>
<li><p>If your precision is low, the F1 is low and if the recall is low again
your F1 score is low.</p></li>
</ul>
</li>
<li><p><strong>Example:</strong> If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.</p></li>
</ul>
</section>
<section id="specificity">
<h3>Specificity<a class="headerlink" href="#specificity" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>True negative = (1 - False Negative Rate)</p></li>
<li><p>Model predicted no and label is False.</p></li>
<li><p><strong>Example:</strong> When the label is False, how often does the model predict False</p></li>
</ul>
</section>
<section id="false-positives">
<h3>False positives<a class="headerlink" href="#false-positives" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>TYPE 1 Error</p></li>
<li><p><strong>Example:</strong> Model predicted yes, but the patient didn‚Äôt actually have the disease.</p></li>
</ul>
</section>
<section id="false-negatives">
<h3>False negatives<a class="headerlink" href="#false-negatives" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>TYPE II Error</p></li>
<li><p><strong>Example:</strong> Model predicted no, but the patient actually did have the disease.</p></li>
</ul>
</section>
<section id="auc-score">
<h3>AUC Score<a class="headerlink" href="#auc-score" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul class="simple">
<li><p>AUC measures the entire two-dimensional area underneath the entire ROC curve ( TPR and FPR)</p></li>
<li><p>It tells how much a model is capable of distinguishing between classes.</p></li>
<li><p>Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.</p></li>
<li><p><strong>Example:</strong> Higher the AUC, then the better the model is at distinguishing between patients with disease and no disease.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="production-monitoring">
<h2>Production Monitoring<a class="headerlink" href="#production-monitoring" title="Permalink to this headline">ÔÉÅ</a></h2>
</section>
<hr class="docutils" />
<section id="a-b-testing">
<h2>A/B Testing<a class="headerlink" href="#a-b-testing" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>A/B Testing is a way of conducting an experiment where you compare a control
group to the performance of one or more test groups by randomly assigning
each group a specific single-variable treatment.</p></li>
</ul>
<p><strong>Experimental Design ‚Üí Control and Control Groups</strong></p>
<ul class="simple">
<li><p>One of the treatments will be the control and the other treatments will be variations on that.</p></li>
<li><p><strong>Determine Power of the Test:</strong></p>
<ul>
<li><p>The minimum sample size for control and experiment groups that yields the highest statistical power (Beta).</p></li>
<li><p>Statistical power increases as alpha increases, thus beta will decrease <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">power</span></code></p></li>
<li><p>Random sampling is a technique where each sample in a population has an equal chance of being chosen.</p></li>
<li><p>Essential that you determine the minimum sample size for your A/B test prior to conducting it so that you can eliminate under coverage bias.</p></li>
</ul>
</li>
</ul>
<p><strong>Hypothesis Testing (T-Test):</strong></p>
<ul class="simple">
<li><p>First, you want to set your alpha, the probability of making a type 1 error. Typically the alpha is set at 5% or 0.05</p></li>
<li><p>Next, you want to determine the probability value (p-value) by first calculating the t-statistic
Lastly, compare the p-value to the alpha. If the p-value is greater than the alpha, do not reject the null!</p></li>
</ul>
</section>
<section id="data-drift">
<h2>Data Drift<a class="headerlink" href="#data-drift" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Drifts involve a statistical change in the data, the best approach to detect them is by monitoring its statistical properties</p></li>
</ul>
<p><strong>Address Model Drift:</strong></p>
<ul class="simple">
<li><p>You could implement a system that periodically trains your models after some
time t, or once it detects a drift  you could refresh a model‚Äôs weight by
extending its training with new data.</p></li>
</ul>
<p><strong>Alerts</strong></p>
<ul class="simple">
<li><p>Recurring, Incremental, Outlier, Abrupt</p></li>
<li><p>Adaptive Sliding Window (very common approach)</p>
<ul>
<li><p>Works by keeping track of several statistical properties of data within a window that automatically grows and shrinks</p></li>
</ul>
</li>
<li><p><strong>Kolmogorov-Smirnov Test</strong></p>
<ul>
<li><p>K-S test is nonparametric, meaning doesn‚Äôt assume any particular underlying distribution</p></li>
<li><p>This test compares your data with a known distribution and test if they come from the same distribution.</p></li>
<li><p>Null Hypothesis</p></li>
<li><p>Samples are from the same distribution</p></li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>