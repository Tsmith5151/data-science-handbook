<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>&lt;no title&gt; &mdash; Data Science Handbook 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Data Science Handbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul class="simple">
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Data Science Handbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>&lt;no title&gt;</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/metrics.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Model Metrics</p>
<p>REGRESSION
R^2
statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable(s) in a regression model.
Problem 1: Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.
Problem 2: If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data.
Leads to overfitting and  produces misleadingly high R-squared values and a lessened ability to make predictions.
Adjusted R^2
modified version of R-squared that has been adjusted for the number of predictors in the model.
Increases only if the new term improves the model more than would be expected by chance.
Decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.<span class="raw-html-m2r"><br></span>
It is always lower than the R-squared.</p>
<p>RMSE
is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values.
Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit.
As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable.
Lower values of RMSE indicate better fit.
RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.</p>
<p>MAE
MAE is not identical to root-mean square error (RMSE), although some researchers report and interpret it that way. MAE is conceptually simpler and also easier to interpret than RMSE: it is simply the average absolute vertical or horizontal distance between each point in a scatter plot and the Y=X line. In other words, MAE is the average absolute difference between X and Y.</p>
<blockquote>
<div><p>m   CLASSIFICATION</p>
</div></blockquote>
<p>Accuracy
Accuracy = (TP+TN)/(TP+FP+FN+TN)
Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.
Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate.
Precision
Positive Predictive Rate
Precision = (TP)/(TP+FP)
what proportion of predicted Positives is truly Positive?
Precision is a valid choice of evaluation metric when we want to be very sure of our prediction</p>
<p>Recall
True Positives or Sensitivity
These are cases in which mode predicted True and label is True
Question: When the label is True, how often does the model predict True?</p>
<p>F1 Score
This is a weighted average of the recall and precision
F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.
Illustration:
If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
Specificity
True negative = (1 - False Negative Rate)
Model predicted no and label is False.
Question: When the label is False, how often does the model predict False</p>
<p>False positives - TYPE 1 Error
Example: Model predicted yes, but the patient didn’t actually have the disease.</p>
<p>False negatives - TYPE II Error
Example: Model predicted no, but the patient actually did have the disease.</p>
<p>AUC Score
AUC measures the entire two-dimensional area underneath the entire ROC curve ( TPR and FPR)
It tells how much a model is capable of distinguishing between classes.
Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.
Example: Higher the AUC, then the better the model is at distinguishing between patients with disease and no disease.</p>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Trace Smith, Damon Resnick.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>